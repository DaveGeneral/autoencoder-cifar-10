{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to train an AutoEncoder for the CIFAR-10 dataset. \n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "### Technology used: Tensorflow-core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for machine learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# packages used for processing: \n",
    "from six.moves import cPickle as pickle # for reading the data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder # for encoding the labels in one hot form\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Models\n",
      "README.md\n",
      "requirements.txt\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data/cifar-10-batches-py\" # the data path\n",
    "train_meta = os.path.join(data_path, \"batches.meta\")\n",
    "base_model_path = '../Models'\n",
    "\n",
    "# constant values:\n",
    "size = 32 # the images of size 32 x 32\n",
    "channels = 3 # RGB channels\n",
    "highest_pixel_value = 255.0 # 8 bits for every channel. So, max value is 255\n",
    "no_of_epochs = 100 # No. of epochs to run\n",
    "no_of_batches = 5 # There are 5 batches in the dataset\n",
    "checkpoint_factor = 5 # save the model after every 5 steps (epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta\n",
      "data_batch_1\n",
      "data_batch_2\n",
      "data_batch_3\n",
      "data_batch_4\n",
      "data_batch_5\n",
      "readme.html\n",
      "test_batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the contents inside the data folder\n",
    "exec_command(['ls', data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unPickle a file: \n",
    "def unpickle(file):\n",
    "    '''\n",
    "        This function takes the file path and unPickles the file acquired from it\n",
    "        @Param file: the string path of the file\n",
    "        @return: The dict object unPickled from the file\n",
    "    '''\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the contents of the batches.meta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_names': ['airplane',\n",
       "  'automobile',\n",
       "  'bird',\n",
       "  'cat',\n",
       "  'deer',\n",
       "  'dog',\n",
       "  'frog',\n",
       "  'horse',\n",
       "  'ship',\n",
       "  'truck'],\n",
       " 'num_cases_per_batch': 10000,\n",
       " 'num_vis': 3072}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = unpickle(train_meta)\n",
    "\n",
    "# check it's contents\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read and display some of the images from the dataset along with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'labels', 'batch_label', 'filenames']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_preliminary = unpickle(os.path.join(data_path, \"data_batch_3\"))\n",
    "\n",
    "# check it's contents\n",
    "train_batch_preliminary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[178, 191, 193, 197, 202, 206, 207, 209, 214, 219],\n",
       "       [140, 151, 155, 160, 166, 172, 173, 171, 176, 180],\n",
       "       [ 84,  94, 119, 151, 146, 127, 125, 135, 139, 139],\n",
       "       [ 16,  18,  85, 200, 207, 133,  71,  59,  72,  79],\n",
       "       [  9,   3,  51, 183, 238, 219, 177,  94,  30,  16],\n",
       "       [ 31,  25,  38, 148, 240, 249, 255, 235, 139,  39],\n",
       "       [ 69,  65,  62, 115, 215, 250, 248, 253, 245, 201],\n",
       "       [ 92,  89,  81,  89, 173, 240, 249, 253, 253, 255],\n",
       "       [ 93,  90,  84,  85, 139, 217, 241, 246, 251, 252],\n",
       "       [ 75,  74,  71,  87, 154, 208, 229, 239, 245, 250]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first 3 images from the dataset\n",
    "preliminary_data = train_batch_preliminary['data'].reshape((len(train_batch_preliminary['data']), 32, 32, 3), \n",
    "                                                           order='F')\n",
    "preliminary_labels = train_batch_preliminary['labels']\n",
    "\n",
    "# view some of the data:\n",
    "preliminary_data[33, :10, :10, 2] #(10 x 10) data of blue channel of 33rd image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 0, 6, 9, 2, 8, 3, 6, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a few values of the labels of the dataset\n",
    "preliminary_labels[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48698c1990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEVCAYAAAAPaTtOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48af6621d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5f8DsXeH7UTOgPMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4865d8d750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "    plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "    plt.imshow(preliminary_data[random_index], interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images look blurred out because they are very low resolution images (32 x 32) pixels only.\n",
    "\n",
    "## It can be seen that the images in the original dataset are skewed. So, we will have to rotate them by 90 degrees clockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nd37HySju/I6TUdz5HSej/H89A2+TgrHKXQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4869657cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try using the numpy.rot90 method for this:\n",
    "random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "plt.imshow(np.rot90(preliminary_data[random_index], axes=(1, 0)), interpolation='none'); # suppress the unnecessary\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works. So, now we can create a function to put all this together. This function would take the batch pickle file and create the data suitable for feeding it off to a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The batch generator function:\n",
    "def generateBatch(batchFile):\n",
    "    '''\n",
    "        The function to generate a batch of data suitable for performing the convNet operations on it\n",
    "        @param batchFile -> the path of the input batchfile\n",
    "        @return batch: (data, labels) -> the processed data.\n",
    "    '''\n",
    "    # unpickle the batch file:\n",
    "    data_dict = unpickle(batchFile)\n",
    "    \n",
    "    # extract the data and labels from this dictionary\n",
    "    unprocessed_data = data_dict['data']\n",
    "    integer_labels = np.array(data_dict['labels']) # labels in integer form\n",
    "    \n",
    "    # reshape and rotate the data\n",
    "    data = unprocessed_data.reshape((len(unprocessed_data), size, size, channels), order='F')\n",
    "    processed_data = np.array(map(lambda x: np.rot90(x, axes=(1, 0)), data))\n",
    "    \n",
    "    # encode the labels in one-hot encoded form\n",
    "    # we use the sklearn.preprocessing package for doing this\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoded_labels = np.array(encoder.fit_transform(integer_labels.reshape(len(integer_labels), 1))).astype(np.int)\n",
    "    \n",
    "    # return the processed data and the encoded_labels:\n",
    "    return (processed_data, encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "# load the batch no. 1 and check if it works correctly.\n",
    "batch_data, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_1\"))\n",
    "print (batch_data.shape, batch_labels.shape)\n",
    "\n",
    "# batch_data[0, :12, :12, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! So, the data extraction module is setup. Let's move on to the actual model building and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The computation graph (tensorflow graph and not the weights) for this model is inside the computation_graph package. It can be imported from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(2), Dimension(2), Dimension(32)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the graph from the Graph1 module \n",
    "import computation_graph.Graph1 \n",
    "\n",
    "computation_graph = computation_graph.Graph1.graph\n",
    "\n",
    "# obtain a handle on the encoded_representation tensor of the dataflow computation graph\n",
    "encoded_representation = computation_graph.get_tensor_by_name(\"encoded_representation:0\")\n",
    "encoded_representation.shape # The output shape of the encoded representation. It is 32 x 2 x 2 i.e 128 \n",
    "# Thus the latent representation is 128 dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(base_model_path, \"Model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's write the session code to run this computation graph and perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Models/Model1/model1-50\n",
      "epoch: 51\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.1345691681\n",
      "range:(2000, 4000) loss= 19.8851413727\n",
      "range:(4000, 6000) loss= 19.7132987976\n",
      "range:(6000, 8000) loss= 19.8943576813\n",
      "range:(8000, 10000) loss= 19.7480754852\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7959842682\n",
      "range:(2000, 4000) loss= 19.8833236694\n",
      "range:(4000, 6000) loss= 19.9237174988\n",
      "range:(6000, 8000) loss= 19.9618320465\n",
      "range:(8000, 10000) loss= 19.9453678131\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.7039737701\n",
      "range:(2000, 4000) loss= 19.9991874695\n",
      "range:(4000, 6000) loss= 19.9202518463\n",
      "range:(6000, 8000) loss= 19.7739391327\n",
      "range:(8000, 10000) loss= 20.1435260773\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9597949982\n",
      "range:(2000, 4000) loss= 19.6442680359\n",
      "range:(4000, 6000) loss= 19.8001041412\n",
      "range:(6000, 8000) loss= 20.1443653107\n",
      "range:(8000, 10000) loss= 19.6835708618\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8838596344\n",
      "range:(2000, 4000) loss= 19.7573356628\n",
      "range:(4000, 6000) loss= 19.8674468994\n",
      "range:(6000, 8000) loss= 19.7158107758\n",
      "range:(8000, 10000) loss= 19.7679653168\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 52\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.1032543182\n",
      "range:(2000, 4000) loss= 19.8553504944\n",
      "range:(4000, 6000) loss= 19.6868133545\n",
      "range:(6000, 8000) loss= 19.8721942902\n",
      "range:(8000, 10000) loss= 19.7308540344\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7842063904\n",
      "range:(2000, 4000) loss= 19.876619339\n",
      "range:(4000, 6000) loss= 19.9204883575\n",
      "range:(6000, 8000) loss= 19.9604625702\n",
      "range:(8000, 10000) loss= 19.9449806213\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.7034435272\n",
      "range:(2000, 4000) loss= 19.9972801208\n",
      "range:(4000, 6000) loss= 19.9174232483\n",
      "range:(6000, 8000) loss= 19.7695922852\n",
      "range:(8000, 10000) loss= 20.1381435394\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9531669617\n",
      "range:(2000, 4000) loss= 19.6373023987\n",
      "range:(4000, 6000) loss= 19.7930316925\n",
      "range:(6000, 8000) loss= 20.1382884979\n",
      "range:(8000, 10000) loss= 19.6782264709\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.879781723\n",
      "range:(2000, 4000) loss= 19.754524231\n",
      "range:(4000, 6000) loss= 19.8656291962\n",
      "range:(6000, 8000) loss= 19.714717865\n",
      "range:(8000, 10000) loss= 19.7672748566\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 53\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.1024875641\n",
      "range:(2000, 4000) loss= 19.8546066284\n",
      "range:(4000, 6000) loss= 19.6858711243\n",
      "range:(6000, 8000) loss= 19.8708000183\n",
      "range:(8000, 10000) loss= 19.7291202545\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7822704315\n",
      "range:(2000, 4000) loss= 19.8747119904\n",
      "range:(4000, 6000) loss= 19.9188671112\n",
      "range:(6000, 8000) loss= 19.9587059021\n",
      "range:(8000, 10000) loss= 19.943315506\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.7019824982\n",
      "range:(2000, 4000) loss= 19.995973587\n",
      "range:(4000, 6000) loss= 19.9164142609\n",
      "range:(6000, 8000) loss= 19.7686367035\n",
      "range:(8000, 10000) loss= 20.1375083923\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9524993896\n",
      "range:(2000, 4000) loss= 19.6366481781\n",
      "range:(4000, 6000) loss= 19.7922859192\n",
      "range:(6000, 8000) loss= 20.1374835968\n",
      "range:(8000, 10000) loss= 19.6774692535\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8789272308\n",
      "range:(2000, 4000) loss= 19.7538013458\n",
      "range:(4000, 6000) loss= 19.8648948669\n",
      "range:(6000, 8000) loss= 19.7139587402\n",
      "range:(8000, 10000) loss= 19.7664451599\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 54\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.1016559601\n",
      "range:(2000, 4000) loss= 19.8538513184\n",
      "range:(4000, 6000) loss= 19.6850986481\n",
      "range:(6000, 8000) loss= 19.8701477051\n",
      "range:(8000, 10000) loss= 19.728433609\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7816486359\n",
      "range:(2000, 4000) loss= 19.87408638\n",
      "range:(4000, 6000) loss= 19.9183216095\n",
      "range:(6000, 8000) loss= 19.9579582214\n",
      "range:(8000, 10000) loss= 19.9426898956\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.7012081146\n",
      "range:(2000, 4000) loss= 19.9953022003\n",
      "range:(4000, 6000) loss= 19.9158172607\n",
      "range:(6000, 8000) loss= 19.7680091858\n",
      "range:(8000, 10000) loss= 20.1368579865\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9518241882\n",
      "range:(2000, 4000) loss= 19.6361026764\n",
      "range:(4000, 6000) loss= 19.7917766571\n",
      "range:(6000, 8000) loss= 20.136964798\n",
      "range:(8000, 10000) loss= 19.676940918\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8783626556\n",
      "range:(2000, 4000) loss= 19.7532653809\n",
      "range:(4000, 6000) loss= 19.8643360138\n",
      "range:(6000, 8000) loss= 19.7133579254\n",
      "range:(8000, 10000) loss= 19.7658843994\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 55\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.1010189056\n",
      "range:(2000, 4000) loss= 19.8532924652\n",
      "range:(4000, 6000) loss= 19.6845207214\n",
      "range:(6000, 8000) loss= 19.8695774078\n",
      "range:(8000, 10000) loss= 19.7277622223\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7809944153\n",
      "range:(2000, 4000) loss= 19.8733978271\n",
      "range:(4000, 6000) loss= 19.9176769257\n",
      "range:(6000, 8000) loss= 19.9573020935\n",
      "range:(8000, 10000) loss= 19.9421348572\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.70063591\n",
      "range:(2000, 4000) loss= 19.9947032928\n",
      "range:(4000, 6000) loss= 19.9152202606\n",
      "range:(6000, 8000) loss= 19.7674274445\n",
      "range:(8000, 10000) loss= 20.1362895966\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9511947632\n",
      "range:(2000, 4000) loss= 19.6355552673\n",
      "range:(4000, 6000) loss= 19.7912578583\n",
      "range:(6000, 8000) loss= 20.1364631653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(8000, 10000) loss= 19.6764163971\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8778343201\n",
      "range:(2000, 4000) loss= 19.7527141571\n",
      "range:(4000, 6000) loss= 19.8637523651\n",
      "range:(6000, 8000) loss= 19.7127952576\n",
      "range:(8000, 10000) loss= 19.765329361\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 56\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.1003837585\n",
      "range:(2000, 4000) loss= 19.8526668549\n",
      "range:(4000, 6000) loss= 19.6839027405\n",
      "range:(6000, 8000) loss= 19.8690166473\n",
      "range:(8000, 10000) loss= 19.7271194458\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.780424118\n",
      "range:(2000, 4000) loss= 19.872844696\n",
      "range:(4000, 6000) loss= 19.9171161652\n",
      "range:(6000, 8000) loss= 19.9567241669\n",
      "range:(8000, 10000) loss= 19.9415225983\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.7000827789\n",
      "range:(2000, 4000) loss= 19.9940853119\n",
      "range:(4000, 6000) loss= 19.9146652222\n",
      "range:(6000, 8000) loss= 19.7668476105\n",
      "range:(8000, 10000) loss= 20.1357440948\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9506549835\n",
      "range:(2000, 4000) loss= 19.6350193024\n",
      "range:(4000, 6000) loss= 19.790725708\n",
      "range:(6000, 8000) loss= 20.1358909607\n",
      "range:(8000, 10000) loss= 19.6758880615\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8773441315\n",
      "range:(2000, 4000) loss= 19.7522335052\n",
      "range:(4000, 6000) loss= 19.8632125854\n",
      "range:(6000, 8000) loss= 19.7122650146\n",
      "range:(8000, 10000) loss= 19.7647361755\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 57\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0997238159\n",
      "range:(2000, 4000) loss= 19.8521060944\n",
      "range:(4000, 6000) loss= 19.6832866669\n",
      "range:(6000, 8000) loss= 19.8684215546\n",
      "range:(8000, 10000) loss= 19.7265319824\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7798652649\n",
      "range:(2000, 4000) loss= 19.8722801208\n",
      "range:(4000, 6000) loss= 19.9165420532\n",
      "range:(6000, 8000) loss= 19.9561538696\n",
      "range:(8000, 10000) loss= 19.9409275055\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6995334625\n",
      "range:(2000, 4000) loss= 19.9934921265\n",
      "range:(4000, 6000) loss= 19.9140777588\n",
      "range:(6000, 8000) loss= 19.7663269043\n",
      "range:(8000, 10000) loss= 20.1352024078\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.950088501\n",
      "range:(2000, 4000) loss= 19.6344432831\n",
      "range:(4000, 6000) loss= 19.7901382446\n",
      "range:(6000, 8000) loss= 20.1353588104\n",
      "range:(8000, 10000) loss= 19.6753940582\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8768234253\n",
      "range:(2000, 4000) loss= 19.751663208\n",
      "range:(4000, 6000) loss= 19.8626766205\n",
      "range:(6000, 8000) loss= 19.7117729187\n",
      "range:(8000, 10000) loss= 19.7642040253\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 58\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0991039276\n",
      "range:(2000, 4000) loss= 19.8514785767\n",
      "range:(4000, 6000) loss= 19.6826782227\n",
      "range:(6000, 8000) loss= 19.8678398132\n",
      "range:(8000, 10000) loss= 19.7259025574\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7793292999\n",
      "range:(2000, 4000) loss= 19.8716773987\n",
      "range:(4000, 6000) loss= 19.9159584045\n",
      "range:(6000, 8000) loss= 19.9555606842\n",
      "range:(8000, 10000) loss= 19.9402980804\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6989688873\n",
      "range:(2000, 4000) loss= 19.9929237366\n",
      "range:(4000, 6000) loss= 19.9134883881\n",
      "range:(6000, 8000) loss= 19.7658119202\n",
      "range:(8000, 10000) loss= 20.1346817017\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9495677948\n",
      "range:(2000, 4000) loss= 19.6339302063\n",
      "range:(4000, 6000) loss= 19.7895679474\n",
      "range:(6000, 8000) loss= 20.134853363\n",
      "range:(8000, 10000) loss= 19.6748657227\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8762626648\n",
      "range:(2000, 4000) loss= 19.7511234283\n",
      "range:(4000, 6000) loss= 19.8621482849\n",
      "range:(6000, 8000) loss= 19.7112083435\n",
      "range:(8000, 10000) loss= 19.7636356354\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 59\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0984973907\n",
      "range:(2000, 4000) loss= 19.8509216309\n",
      "range:(4000, 6000) loss= 19.6820831299\n",
      "range:(6000, 8000) loss= 19.867275238\n",
      "range:(8000, 10000) loss= 19.7253093719\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7787322998\n",
      "range:(2000, 4000) loss= 19.8710594177\n",
      "range:(4000, 6000) loss= 19.9154262543\n",
      "range:(6000, 8000) loss= 19.9549598694\n",
      "range:(8000, 10000) loss= 19.939743042\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6983833313\n",
      "range:(2000, 4000) loss= 19.9923706055\n",
      "range:(4000, 6000) loss= 19.9129486084\n",
      "range:(6000, 8000) loss= 19.7652873993\n",
      "range:(8000, 10000) loss= 20.1341133118\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9489784241\n",
      "range:(2000, 4000) loss= 19.6333770752\n",
      "range:(4000, 6000) loss= 19.7890129089\n",
      "range:(6000, 8000) loss= 20.1342887878\n",
      "range:(8000, 10000) loss= 19.6743183136\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8756999969\n",
      "range:(2000, 4000) loss= 19.7505455017\n",
      "range:(4000, 6000) loss= 19.8616085052\n",
      "range:(6000, 8000) loss= 19.7106227875\n",
      "range:(8000, 10000) loss= 19.7630443573\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 60\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0978507996\n",
      "range:(2000, 4000) loss= 19.8503742218\n",
      "range:(4000, 6000) loss= 19.6814804077\n",
      "range:(6000, 8000) loss= 19.8667030334\n",
      "range:(8000, 10000) loss= 19.7247409821\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7781524658\n",
      "range:(2000, 4000) loss= 19.870470047\n",
      "range:(4000, 6000) loss= 19.9148292542\n",
      "range:(6000, 8000) loss= 19.9543609619\n",
      "range:(8000, 10000) loss= 19.9391860962\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6978149414\n",
      "range:(2000, 4000) loss= 19.9918346405\n",
      "range:(4000, 6000) loss= 19.912361145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(6000, 8000) loss= 19.7647418976\n",
      "range:(8000, 10000) loss= 20.1335506439\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9483470917\n",
      "range:(2000, 4000) loss= 19.6328315735\n",
      "range:(4000, 6000) loss= 19.7884159088\n",
      "range:(6000, 8000) loss= 20.1336917877\n",
      "range:(8000, 10000) loss= 19.6737480164\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8751392365\n",
      "range:(2000, 4000) loss= 19.749956131\n",
      "range:(4000, 6000) loss= 19.8610153198\n",
      "range:(6000, 8000) loss= 19.7100334167\n",
      "range:(8000, 10000) loss= 19.7624664307\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 61\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0972518921\n",
      "range:(2000, 4000) loss= 19.8497409821\n",
      "range:(4000, 6000) loss= 19.6809062958\n",
      "range:(6000, 8000) loss= 19.8661289215\n",
      "range:(8000, 10000) loss= 19.7241153717\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7775363922\n",
      "range:(2000, 4000) loss= 19.8698501587\n",
      "range:(4000, 6000) loss= 19.9142894745\n",
      "range:(6000, 8000) loss= 19.9537639618\n",
      "range:(8000, 10000) loss= 19.9386234283\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6972370148\n",
      "range:(2000, 4000) loss= 19.9912338257\n",
      "range:(4000, 6000) loss= 19.9117660522\n",
      "range:(6000, 8000) loss= 19.764175415\n",
      "range:(8000, 10000) loss= 20.1329917908\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9477672577\n",
      "range:(2000, 4000) loss= 19.632270813\n",
      "range:(4000, 6000) loss= 19.7878284454\n",
      "range:(6000, 8000) loss= 20.133108139\n",
      "range:(8000, 10000) loss= 19.6732025146\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.874584198\n",
      "range:(2000, 4000) loss= 19.7493896484\n",
      "range:(4000, 6000) loss= 19.8604183197\n",
      "range:(6000, 8000) loss= 19.7094554901\n",
      "range:(8000, 10000) loss= 19.761882782\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 62\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0966567993\n",
      "range:(2000, 4000) loss= 19.8491897583\n",
      "range:(4000, 6000) loss= 19.6802864075\n",
      "range:(6000, 8000) loss= 19.8655567169\n",
      "range:(8000, 10000) loss= 19.7234802246\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7769355774\n",
      "range:(2000, 4000) loss= 19.8692703247\n",
      "range:(4000, 6000) loss= 19.9137134552\n",
      "range:(6000, 8000) loss= 19.953168869\n",
      "range:(8000, 10000) loss= 19.9380207062\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6966629028\n",
      "range:(2000, 4000) loss= 19.9906997681\n",
      "range:(4000, 6000) loss= 19.9111843109\n",
      "range:(6000, 8000) loss= 19.7636146545\n",
      "range:(8000, 10000) loss= 20.1323490143\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9471988678\n",
      "range:(2000, 4000) loss= 19.6316947937\n",
      "range:(4000, 6000) loss= 19.7872600555\n",
      "range:(6000, 8000) loss= 20.1325378418\n",
      "range:(8000, 10000) loss= 19.6726398468\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8739643097\n",
      "range:(2000, 4000) loss= 19.7487926483\n",
      "range:(4000, 6000) loss= 19.8598461151\n",
      "range:(6000, 8000) loss= 19.7088890076\n",
      "range:(8000, 10000) loss= 19.761302948\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 63\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0960388184\n",
      "range:(2000, 4000) loss= 19.8486022949\n",
      "range:(4000, 6000) loss= 19.6796550751\n",
      "range:(6000, 8000) loss= 19.8649520874\n",
      "range:(8000, 10000) loss= 19.7228660583\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7763233185\n",
      "range:(2000, 4000) loss= 19.8686714172\n",
      "range:(4000, 6000) loss= 19.9131183624\n",
      "range:(6000, 8000) loss= 19.9525699615\n",
      "range:(8000, 10000) loss= 19.9373989105\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6960506439\n",
      "range:(2000, 4000) loss= 19.990114212\n",
      "range:(4000, 6000) loss= 19.9105796814\n",
      "range:(6000, 8000) loss= 19.763048172\n",
      "range:(8000, 10000) loss= 20.1317672729\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9466381073\n",
      "range:(2000, 4000) loss= 19.6311130524\n",
      "range:(4000, 6000) loss= 19.7866954803\n",
      "range:(6000, 8000) loss= 20.1319923401\n",
      "range:(8000, 10000) loss= 19.6720752716\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8733901978\n",
      "range:(2000, 4000) loss= 19.7481594086\n",
      "range:(4000, 6000) loss= 19.8592681885\n",
      "range:(6000, 8000) loss= 19.7082958221\n",
      "range:(8000, 10000) loss= 19.7607002258\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 64\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0953845978\n",
      "range:(2000, 4000) loss= 19.8480262756\n",
      "range:(4000, 6000) loss= 19.6790466309\n",
      "range:(6000, 8000) loss= 19.8643169403\n",
      "range:(8000, 10000) loss= 19.7222290039\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7757015228\n",
      "range:(2000, 4000) loss= 19.8680744171\n",
      "range:(4000, 6000) loss= 19.9125232697\n",
      "range:(6000, 8000) loss= 19.952009201\n",
      "range:(8000, 10000) loss= 19.9368076324\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6954231262\n",
      "range:(2000, 4000) loss= 19.9894828796\n",
      "range:(4000, 6000) loss= 19.9100017548\n",
      "range:(6000, 8000) loss= 19.7624607086\n",
      "range:(8000, 10000) loss= 20.1311206818\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.946023941\n",
      "range:(2000, 4000) loss= 19.6305160522\n",
      "range:(4000, 6000) loss= 19.7861042023\n",
      "range:(6000, 8000) loss= 20.1313858032\n",
      "range:(8000, 10000) loss= 19.6715145111\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8728199005\n",
      "range:(2000, 4000) loss= 19.747549057\n",
      "range:(4000, 6000) loss= 19.8586788177\n",
      "range:(6000, 8000) loss= 19.7076911926\n",
      "range:(8000, 10000) loss= 19.7600688934\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 65\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0947418213\n",
      "range:(2000, 4000) loss= 19.8474216461\n",
      "range:(4000, 6000) loss= 19.678440094\n",
      "range:(6000, 8000) loss= 19.8636875153\n",
      "range:(8000, 10000) loss= 19.7215862274\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7751045227\n",
      "range:(2000, 4000) loss= 19.8674411774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(4000, 6000) loss= 19.9119129181\n",
      "range:(6000, 8000) loss= 19.951417923\n",
      "range:(8000, 10000) loss= 19.9361934662\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6948451996\n",
      "range:(2000, 4000) loss= 19.9888820648\n",
      "range:(4000, 6000) loss= 19.9093818665\n",
      "range:(6000, 8000) loss= 19.7618579865\n",
      "range:(8000, 10000) loss= 20.1305294037\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9454402924\n",
      "range:(2000, 4000) loss= 19.6299285889\n",
      "range:(4000, 6000) loss= 19.7854824066\n",
      "range:(6000, 8000) loss= 20.1307849884\n",
      "range:(8000, 10000) loss= 19.6709251404\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8722190857\n",
      "range:(2000, 4000) loss= 19.7469043732\n",
      "range:(4000, 6000) loss= 19.8580532074\n",
      "range:(6000, 8000) loss= 19.7070884705\n",
      "range:(8000, 10000) loss= 19.7594470978\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 66\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0941181183\n",
      "range:(2000, 4000) loss= 19.8467693329\n",
      "range:(4000, 6000) loss= 19.6777915955\n",
      "range:(6000, 8000) loss= 19.8630409241\n",
      "range:(8000, 10000) loss= 19.7209606171\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7744541168\n",
      "range:(2000, 4000) loss= 19.8667850494\n",
      "range:(4000, 6000) loss= 19.9112968445\n",
      "range:(6000, 8000) loss= 19.9507961273\n",
      "range:(8000, 10000) loss= 19.9355354309\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6942386627\n",
      "range:(2000, 4000) loss= 19.9882736206\n",
      "range:(4000, 6000) loss= 19.9087963104\n",
      "range:(6000, 8000) loss= 19.7612628937\n",
      "range:(8000, 10000) loss= 20.129901886\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9448299408\n",
      "range:(2000, 4000) loss= 19.6293373108\n",
      "range:(4000, 6000) loss= 19.7848644257\n",
      "range:(6000, 8000) loss= 20.1301841736\n",
      "range:(8000, 10000) loss= 19.6703224182\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8715953827\n",
      "range:(2000, 4000) loss= 19.7463150024\n",
      "range:(4000, 6000) loss= 19.8574390411\n",
      "range:(6000, 8000) loss= 19.7064590454\n",
      "range:(8000, 10000) loss= 19.758852005\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 67\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0934791565\n",
      "range:(2000, 4000) loss= 19.8461418152\n",
      "range:(4000, 6000) loss= 19.6771678925\n",
      "range:(6000, 8000) loss= 19.8624229431\n",
      "range:(8000, 10000) loss= 19.7202796936\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7737808228\n",
      "range:(2000, 4000) loss= 19.8661403656\n",
      "range:(4000, 6000) loss= 19.9106616974\n",
      "range:(6000, 8000) loss= 19.9501590729\n",
      "range:(8000, 10000) loss= 19.9349098206\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6936168671\n",
      "range:(2000, 4000) loss= 19.9876823425\n",
      "range:(4000, 6000) loss= 19.9081726074\n",
      "range:(6000, 8000) loss= 19.7606658936\n",
      "range:(8000, 10000) loss= 20.1293010712\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.944190979\n",
      "range:(2000, 4000) loss= 19.6287078857\n",
      "range:(4000, 6000) loss= 19.78424263\n",
      "range:(6000, 8000) loss= 20.1295814514\n",
      "range:(8000, 10000) loss= 19.6697406769\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8709716797\n",
      "range:(2000, 4000) loss= 19.7456665039\n",
      "range:(4000, 6000) loss= 19.8568401337\n",
      "range:(6000, 8000) loss= 19.7057914734\n",
      "range:(8000, 10000) loss= 19.7582092285\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 68\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0928192139\n",
      "range:(2000, 4000) loss= 19.8455085754\n",
      "range:(4000, 6000) loss= 19.6765327454\n",
      "range:(6000, 8000) loss= 19.8617858887\n",
      "range:(8000, 10000) loss= 19.7195911407\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7731533051\n",
      "range:(2000, 4000) loss= 19.8655185699\n",
      "range:(4000, 6000) loss= 19.9100379944\n",
      "range:(6000, 8000) loss= 19.9495182037\n",
      "range:(8000, 10000) loss= 19.9342956543\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.692987442\n",
      "range:(2000, 4000) loss= 19.9870681763\n",
      "range:(4000, 6000) loss= 19.9075393677\n",
      "range:(6000, 8000) loss= 19.7600498199\n",
      "range:(8000, 10000) loss= 20.1286640167\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9435348511\n",
      "range:(2000, 4000) loss= 19.6280975342\n",
      "range:(4000, 6000) loss= 19.7836036682\n",
      "range:(6000, 8000) loss= 20.128950119\n",
      "range:(8000, 10000) loss= 19.6691265106\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8703517914\n",
      "range:(2000, 4000) loss= 19.7450618744\n",
      "range:(4000, 6000) loss= 19.8562469482\n",
      "range:(6000, 8000) loss= 19.7051620483\n",
      "range:(8000, 10000) loss= 19.7575511932\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 69\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0921859741\n",
      "range:(2000, 4000) loss= 19.844877243\n",
      "range:(4000, 6000) loss= 19.6759128571\n",
      "range:(6000, 8000) loss= 19.8611221313\n",
      "range:(8000, 10000) loss= 19.7188987732\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7725028992\n",
      "range:(2000, 4000) loss= 19.8648509979\n",
      "range:(4000, 6000) loss= 19.9093799591\n",
      "range:(6000, 8000) loss= 19.9488620758\n",
      "range:(8000, 10000) loss= 19.9336547852\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6923465729\n",
      "range:(2000, 4000) loss= 19.9864006042\n",
      "range:(4000, 6000) loss= 19.9069042206\n",
      "range:(6000, 8000) loss= 19.7594070435\n",
      "range:(8000, 10000) loss= 20.1280155182\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9428672791\n",
      "range:(2000, 4000) loss= 19.6274738312\n",
      "range:(4000, 6000) loss= 19.7829437256\n",
      "range:(6000, 8000) loss= 20.1283092499\n",
      "range:(8000, 10000) loss= 19.668466568\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8696937561\n",
      "range:(2000, 4000) loss= 19.7444057465\n",
      "range:(4000, 6000) loss= 19.8556365967\n",
      "range:(6000, 8000) loss= 19.7044715881\n",
      "range:(8000, 10000) loss= 19.7568778992\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 70\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(0, 2000) loss= 20.0915336609\n",
      "range:(2000, 4000) loss= 19.8442077637\n",
      "range:(4000, 6000) loss= 19.6752357483\n",
      "range:(6000, 8000) loss= 19.8604640961\n",
      "range:(8000, 10000) loss= 19.7182350159\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7718410492\n",
      "range:(2000, 4000) loss= 19.8641872406\n",
      "range:(4000, 6000) loss= 19.9086875916\n",
      "range:(6000, 8000) loss= 19.9482307434\n",
      "range:(8000, 10000) loss= 19.933013916\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.691701889\n",
      "range:(2000, 4000) loss= 19.9857616425\n",
      "range:(4000, 6000) loss= 19.9062347412\n",
      "range:(6000, 8000) loss= 19.7587509155\n",
      "range:(8000, 10000) loss= 20.1273403168\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9422187805\n",
      "range:(2000, 4000) loss= 19.6268291473\n",
      "range:(4000, 6000) loss= 19.7822914124\n",
      "range:(6000, 8000) loss= 20.1276245117\n",
      "range:(8000, 10000) loss= 19.6678504944\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8690299988\n",
      "range:(2000, 4000) loss= 19.7437610626\n",
      "range:(4000, 6000) loss= 19.855009079\n",
      "range:(6000, 8000) loss= 19.7038402557\n",
      "range:(8000, 10000) loss= 19.7562294006\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 71\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0908489227\n",
      "range:(2000, 4000) loss= 19.8435688019\n",
      "range:(4000, 6000) loss= 19.6746082306\n",
      "range:(6000, 8000) loss= 19.8597640991\n",
      "range:(8000, 10000) loss= 19.717540741\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7711601257\n",
      "range:(2000, 4000) loss= 19.8634815216\n",
      "range:(4000, 6000) loss= 19.9080352783\n",
      "range:(6000, 8000) loss= 19.9475708008\n",
      "range:(8000, 10000) loss= 19.9323501587\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.691072464\n",
      "range:(2000, 4000) loss= 19.985086441\n",
      "range:(4000, 6000) loss= 19.9055614471\n",
      "range:(6000, 8000) loss= 19.7580928802\n",
      "range:(8000, 10000) loss= 20.126663208\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9415397644\n",
      "range:(2000, 4000) loss= 19.626203537\n",
      "range:(4000, 6000) loss= 19.7816123962\n",
      "range:(6000, 8000) loss= 20.1269187927\n",
      "range:(8000, 10000) loss= 19.6672000885\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8683681488\n",
      "range:(2000, 4000) loss= 19.7430973053\n",
      "range:(4000, 6000) loss= 19.8543701172\n",
      "range:(6000, 8000) loss= 19.7031497955\n",
      "range:(8000, 10000) loss= 19.7555809021\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 72\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.090133667\n",
      "range:(2000, 4000) loss= 19.842918396\n",
      "range:(4000, 6000) loss= 19.6739330292\n",
      "range:(6000, 8000) loss= 19.8590755463\n",
      "range:(8000, 10000) loss= 19.7168483734\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7704524994\n",
      "range:(2000, 4000) loss= 19.8627796173\n",
      "range:(4000, 6000) loss= 19.9073925018\n",
      "range:(6000, 8000) loss= 19.9468898773\n",
      "range:(8000, 10000) loss= 19.9317054749\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6904239655\n",
      "range:(2000, 4000) loss= 19.984418869\n",
      "range:(4000, 6000) loss= 19.9048919678\n",
      "range:(6000, 8000) loss= 19.7574310303\n",
      "range:(8000, 10000) loss= 20.1259841919\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.940864563\n",
      "range:(2000, 4000) loss= 19.625535965\n",
      "range:(4000, 6000) loss= 19.7809410095\n",
      "range:(6000, 8000) loss= 20.1262283325\n",
      "range:(8000, 10000) loss= 19.6665668488\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8676948547\n",
      "range:(2000, 4000) loss= 19.7424201965\n",
      "range:(4000, 6000) loss= 19.8537311554\n",
      "range:(6000, 8000) loss= 19.7024917603\n",
      "range:(8000, 10000) loss= 19.7549285889\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 73\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0894145966\n",
      "range:(2000, 4000) loss= 19.8422451019\n",
      "range:(4000, 6000) loss= 19.6732788086\n",
      "range:(6000, 8000) loss= 19.8583984375\n",
      "range:(8000, 10000) loss= 19.7161483765\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7697505951\n",
      "range:(2000, 4000) loss= 19.8620872498\n",
      "range:(4000, 6000) loss= 19.9066963196\n",
      "range:(6000, 8000) loss= 19.9462108612\n",
      "range:(8000, 10000) loss= 19.9310626984\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6897583008\n",
      "range:(2000, 4000) loss= 19.9837532043\n",
      "range:(4000, 6000) loss= 19.9041976929\n",
      "range:(6000, 8000) loss= 19.7567577362\n",
      "range:(8000, 10000) loss= 20.1252861023\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9401817322\n",
      "range:(2000, 4000) loss= 19.6249065399\n",
      "range:(4000, 6000) loss= 19.7802715302\n",
      "range:(6000, 8000) loss= 20.1255397797\n",
      "range:(8000, 10000) loss= 19.6658821106\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8670482635\n",
      "range:(2000, 4000) loss= 19.7417259216\n",
      "range:(4000, 6000) loss= 19.8530368805\n",
      "range:(6000, 8000) loss= 19.7018222809\n",
      "range:(8000, 10000) loss= 19.754240036\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 74\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0887088776\n",
      "range:(2000, 4000) loss= 19.8415622711\n",
      "range:(4000, 6000) loss= 19.6725959778\n",
      "range:(6000, 8000) loss= 19.857711792\n",
      "range:(8000, 10000) loss= 19.7154426575\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.76902771\n",
      "range:(2000, 4000) loss= 19.8613872528\n",
      "range:(4000, 6000) loss= 19.9059886932\n",
      "range:(6000, 8000) loss= 19.9455451965\n",
      "range:(8000, 10000) loss= 19.9303874969\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.689081192\n",
      "range:(2000, 4000) loss= 19.9830741882\n",
      "range:(4000, 6000) loss= 19.9035167694\n",
      "range:(6000, 8000) loss= 19.7560977936\n",
      "range:(8000, 10000) loss= 20.1246242523\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9395236969\n",
      "range:(2000, 4000) loss= 19.6242446899\n",
      "range:(4000, 6000) loss= 19.7795658112\n",
      "range:(6000, 8000) loss= 20.1248474121\n",
      "range:(8000, 10000) loss= 19.6651916504\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8663959503\n",
      "range:(2000, 4000) loss= 19.7410240173\n",
      "range:(4000, 6000) loss= 19.8523483276\n",
      "range:(6000, 8000) loss= 19.7011375427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(8000, 10000) loss= 19.7535858154\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 75\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0879917145\n",
      "range:(2000, 4000) loss= 19.8408660889\n",
      "range:(4000, 6000) loss= 19.6719017029\n",
      "range:(6000, 8000) loss= 19.8570156097\n",
      "range:(8000, 10000) loss= 19.7147426605\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7683296204\n",
      "range:(2000, 4000) loss= 19.8606948853\n",
      "range:(4000, 6000) loss= 19.9053249359\n",
      "range:(6000, 8000) loss= 19.9448604584\n",
      "range:(8000, 10000) loss= 19.9296722412\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6884117126\n",
      "range:(2000, 4000) loss= 19.9824180603\n",
      "range:(4000, 6000) loss= 19.9028358459\n",
      "range:(6000, 8000) loss= 19.7553863525\n",
      "range:(8000, 10000) loss= 20.1238861084\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9388103485\n",
      "range:(2000, 4000) loss= 19.6235122681\n",
      "range:(4000, 6000) loss= 19.7788486481\n",
      "range:(6000, 8000) loss= 20.1241359711\n",
      "range:(8000, 10000) loss= 19.6645336151\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8656730652\n",
      "range:(2000, 4000) loss= 19.7403030396\n",
      "range:(4000, 6000) loss= 19.8516330719\n",
      "range:(6000, 8000) loss= 19.7004127502\n",
      "range:(8000, 10000) loss= 19.7529182434\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 76\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0872764587\n",
      "range:(2000, 4000) loss= 19.8401203156\n",
      "range:(4000, 6000) loss= 19.6711978912\n",
      "range:(6000, 8000) loss= 19.8562927246\n",
      "range:(8000, 10000) loss= 19.7140102386\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7676086426\n",
      "range:(2000, 4000) loss= 19.8599624634\n",
      "range:(4000, 6000) loss= 19.9045944214\n",
      "range:(6000, 8000) loss= 19.944152832\n",
      "range:(8000, 10000) loss= 19.9289722443\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.687707901\n",
      "range:(2000, 4000) loss= 19.9817199707\n",
      "range:(4000, 6000) loss= 19.9021263123\n",
      "range:(6000, 8000) loss= 19.7546882629\n",
      "range:(8000, 10000) loss= 20.1231594086\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9381046295\n",
      "range:(2000, 4000) loss= 19.6228275299\n",
      "range:(4000, 6000) loss= 19.7781124115\n",
      "range:(6000, 8000) loss= 20.1234054565\n",
      "range:(8000, 10000) loss= 19.6638259888\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8649654388\n",
      "range:(2000, 4000) loss= 19.739566803\n",
      "range:(4000, 6000) loss= 19.8509426117\n",
      "range:(6000, 8000) loss= 19.699678421\n",
      "range:(8000, 10000) loss= 19.7521877289\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 77\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0865516663\n",
      "range:(2000, 4000) loss= 19.8393859863\n",
      "range:(4000, 6000) loss= 19.6704902649\n",
      "range:(6000, 8000) loss= 19.8555374146\n",
      "range:(8000, 10000) loss= 19.7132778168\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7668781281\n",
      "range:(2000, 4000) loss= 19.8592357635\n",
      "range:(4000, 6000) loss= 19.903886795\n",
      "range:(6000, 8000) loss= 19.9434356689\n",
      "range:(8000, 10000) loss= 19.9282894135\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6870269775\n",
      "range:(2000, 4000) loss= 19.9809837341\n",
      "range:(4000, 6000) loss= 19.9014625549\n",
      "range:(6000, 8000) loss= 19.7539596558\n",
      "range:(8000, 10000) loss= 20.1224536896\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9373950958\n",
      "range:(2000, 4000) loss= 19.6221580505\n",
      "range:(4000, 6000) loss= 19.7773933411\n",
      "range:(6000, 8000) loss= 20.12266922\n",
      "range:(8000, 10000) loss= 19.663105011\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8642406464\n",
      "range:(2000, 4000) loss= 19.7388496399\n",
      "range:(4000, 6000) loss= 19.8502388\n",
      "range:(6000, 8000) loss= 19.6989574432\n",
      "range:(8000, 10000) loss= 19.7514953613\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 78\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0858001709\n",
      "range:(2000, 4000) loss= 19.8386974335\n",
      "range:(4000, 6000) loss= 19.6697616577\n",
      "range:(6000, 8000) loss= 19.8548107147\n",
      "range:(8000, 10000) loss= 19.7124958038\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7661170959\n",
      "range:(2000, 4000) loss= 19.8585243225\n",
      "range:(4000, 6000) loss= 19.9031600952\n",
      "range:(6000, 8000) loss= 19.9427127838\n",
      "range:(8000, 10000) loss= 19.9275455475\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6863288879\n",
      "range:(2000, 4000) loss= 19.9802780151\n",
      "range:(4000, 6000) loss= 19.9007377625\n",
      "range:(6000, 8000) loss= 19.7532024384\n",
      "range:(8000, 10000) loss= 20.1217498779\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9366550446\n",
      "range:(2000, 4000) loss= 19.6214847565\n",
      "range:(4000, 6000) loss= 19.7766532898\n",
      "range:(6000, 8000) loss= 20.1219463348\n",
      "range:(8000, 10000) loss= 19.6624336243\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8635063171\n",
      "range:(2000, 4000) loss= 19.7380828857\n",
      "range:(4000, 6000) loss= 19.849521637\n",
      "range:(6000, 8000) loss= 19.6981983185\n",
      "range:(8000, 10000) loss= 19.7507858276\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 79\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0850467682\n",
      "range:(2000, 4000) loss= 19.8379611969\n",
      "range:(4000, 6000) loss= 19.6690425873\n",
      "range:(6000, 8000) loss= 19.8540611267\n",
      "range:(8000, 10000) loss= 19.7117767334\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7653675079\n",
      "range:(2000, 4000) loss= 19.8577651978\n",
      "range:(4000, 6000) loss= 19.9024276733\n",
      "range:(6000, 8000) loss= 19.9419803619\n",
      "range:(8000, 10000) loss= 19.9268188477\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6856098175\n",
      "range:(2000, 4000) loss= 19.979549408\n",
      "range:(4000, 6000) loss= 19.8999824524\n",
      "range:(6000, 8000) loss= 19.7524547577\n",
      "range:(8000, 10000) loss= 20.1210002899\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9359264374\n",
      "range:(2000, 4000) loss= 19.6207904816\n",
      "range:(4000, 6000) loss= 19.7759037018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(6000, 8000) loss= 20.1211891174\n",
      "range:(8000, 10000) loss= 19.6617126465\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8627738953\n",
      "range:(2000, 4000) loss= 19.7373371124\n",
      "range:(4000, 6000) loss= 19.8487987518\n",
      "range:(6000, 8000) loss= 19.6974525452\n",
      "range:(8000, 10000) loss= 19.7500610352\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 80\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0843009949\n",
      "range:(2000, 4000) loss= 19.8372306824\n",
      "range:(4000, 6000) loss= 19.6682777405\n",
      "range:(6000, 8000) loss= 19.8533000946\n",
      "range:(8000, 10000) loss= 19.7110271454\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7646389008\n",
      "range:(2000, 4000) loss= 19.857006073\n",
      "range:(4000, 6000) loss= 19.901720047\n",
      "range:(6000, 8000) loss= 19.9412403107\n",
      "range:(8000, 10000) loss= 19.9260978699\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6848831177\n",
      "range:(2000, 4000) loss= 19.978811264\n",
      "range:(4000, 6000) loss= 19.8992481232\n",
      "range:(6000, 8000) loss= 19.7517127991\n",
      "range:(8000, 10000) loss= 20.1202716827\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9351806641\n",
      "range:(2000, 4000) loss= 19.6200962067\n",
      "range:(4000, 6000) loss= 19.7751789093\n",
      "range:(6000, 8000) loss= 20.1204166412\n",
      "range:(8000, 10000) loss= 19.6609859467\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8619995117\n",
      "range:(2000, 4000) loss= 19.7365951538\n",
      "range:(4000, 6000) loss= 19.8480644226\n",
      "range:(6000, 8000) loss= 19.6967029572\n",
      "range:(8000, 10000) loss= 19.7493038177\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 81\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0835056305\n",
      "range:(2000, 4000) loss= 19.8364810944\n",
      "range:(4000, 6000) loss= 19.667547226\n",
      "range:(6000, 8000) loss= 19.8525276184\n",
      "range:(8000, 10000) loss= 19.710269928\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7638664246\n",
      "range:(2000, 4000) loss= 19.8562316895\n",
      "range:(4000, 6000) loss= 19.9009723663\n",
      "range:(6000, 8000) loss= 19.9404850006\n",
      "range:(8000, 10000) loss= 19.9253444672\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6841392517\n",
      "range:(2000, 4000) loss= 19.978094101\n",
      "range:(4000, 6000) loss= 19.8985157013\n",
      "range:(6000, 8000) loss= 19.7509498596\n",
      "range:(8000, 10000) loss= 20.1194972992\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9344005585\n",
      "range:(2000, 4000) loss= 19.619392395\n",
      "range:(4000, 6000) loss= 19.7743759155\n",
      "range:(6000, 8000) loss= 20.1196365356\n",
      "range:(8000, 10000) loss= 19.6602249146\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8612289429\n",
      "range:(2000, 4000) loss= 19.7358455658\n",
      "range:(4000, 6000) loss= 19.8473358154\n",
      "range:(6000, 8000) loss= 19.6959190369\n",
      "range:(8000, 10000) loss= 19.7485694885\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 82\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0826931\n",
      "range:(2000, 4000) loss= 19.8357048035\n",
      "range:(4000, 6000) loss= 19.6667518616\n",
      "range:(6000, 8000) loss= 19.85181427\n",
      "range:(8000, 10000) loss= 19.7095012665\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7630901337\n",
      "range:(2000, 4000) loss= 19.8554401398\n",
      "range:(4000, 6000) loss= 19.9002094269\n",
      "range:(6000, 8000) loss= 19.939699173\n",
      "range:(8000, 10000) loss= 19.924577713\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6833667755\n",
      "range:(2000, 4000) loss= 19.9773483276\n",
      "range:(4000, 6000) loss= 19.8977394104\n",
      "range:(6000, 8000) loss= 19.750202179\n",
      "range:(8000, 10000) loss= 20.118724823\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9336242676\n",
      "range:(2000, 4000) loss= 19.6186542511\n",
      "range:(4000, 6000) loss= 19.7735824585\n",
      "range:(6000, 8000) loss= 20.1188583374\n",
      "range:(8000, 10000) loss= 19.6595115662\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.860458374\n",
      "range:(2000, 4000) loss= 19.7350292206\n",
      "range:(4000, 6000) loss= 19.846572876\n",
      "range:(6000, 8000) loss= 19.6951370239\n",
      "range:(8000, 10000) loss= 19.7478427887\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 83\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0818958282\n",
      "range:(2000, 4000) loss= 19.834941864\n",
      "range:(4000, 6000) loss= 19.6659793854\n",
      "range:(6000, 8000) loss= 19.85105896\n",
      "range:(8000, 10000) loss= 19.7087287903\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7623252869\n",
      "range:(2000, 4000) loss= 19.8546619415\n",
      "range:(4000, 6000) loss= 19.8994178772\n",
      "range:(6000, 8000) loss= 19.9389419556\n",
      "range:(8000, 10000) loss= 19.9238300323\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6826152802\n",
      "range:(2000, 4000) loss= 19.9765968323\n",
      "range:(4000, 6000) loss= 19.8969764709\n",
      "range:(6000, 8000) loss= 19.7494430542\n",
      "range:(8000, 10000) loss= 20.1179695129\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9328632355\n",
      "range:(2000, 4000) loss= 19.6179351807\n",
      "range:(4000, 6000) loss= 19.7728404999\n",
      "range:(6000, 8000) loss= 20.1180591583\n",
      "range:(8000, 10000) loss= 19.6587276459\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8596763611\n",
      "range:(2000, 4000) loss= 19.7342453003\n",
      "range:(4000, 6000) loss= 19.8458042145\n",
      "range:(6000, 8000) loss= 19.6943893433\n",
      "range:(8000, 10000) loss= 19.747089386\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 84\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0811023712\n",
      "range:(2000, 4000) loss= 19.8341674805\n",
      "range:(4000, 6000) loss= 19.665184021\n",
      "range:(6000, 8000) loss= 19.850227356\n",
      "range:(8000, 10000) loss= 19.7079582214\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.761554718\n",
      "range:(2000, 4000) loss= 19.8538665771\n",
      "range:(4000, 6000) loss= 19.8986339569\n",
      "range:(6000, 8000) loss= 19.9381427765\n",
      "range:(8000, 10000) loss= 19.923034668\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6818332672\n",
      "range:(2000, 4000) loss= 19.9758358002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(4000, 6000) loss= 19.896156311\n",
      "range:(6000, 8000) loss= 19.7486286163\n",
      "range:(8000, 10000) loss= 20.1171684265\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9320888519\n",
      "range:(2000, 4000) loss= 19.6171894073\n",
      "range:(4000, 6000) loss= 19.7720241547\n",
      "range:(6000, 8000) loss= 20.1172409058\n",
      "range:(8000, 10000) loss= 19.6579399109\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.858915329\n",
      "range:(2000, 4000) loss= 19.7334461212\n",
      "range:(4000, 6000) loss= 19.8449840546\n",
      "range:(6000, 8000) loss= 19.6935825348\n",
      "range:(8000, 10000) loss= 19.7463150024\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 85\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0802936554\n",
      "range:(2000, 4000) loss= 19.8334064484\n",
      "range:(4000, 6000) loss= 19.6644058228\n",
      "range:(6000, 8000) loss= 19.8493995667\n",
      "range:(8000, 10000) loss= 19.7071609497\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7607479095\n",
      "range:(2000, 4000) loss= 19.8530445099\n",
      "range:(4000, 6000) loss= 19.8978652954\n",
      "range:(6000, 8000) loss= 19.9373569489\n",
      "range:(8000, 10000) loss= 19.922252655\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6810913086\n",
      "range:(2000, 4000) loss= 19.9750900269\n",
      "range:(4000, 6000) loss= 19.895362854\n",
      "range:(6000, 8000) loss= 19.7478218079\n",
      "range:(8000, 10000) loss= 20.1163368225\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9312858582\n",
      "range:(2000, 4000) loss= 19.6164283752\n",
      "range:(4000, 6000) loss= 19.7712364197\n",
      "range:(6000, 8000) loss= 20.1164054871\n",
      "range:(8000, 10000) loss= 19.6571826935\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8581237793\n",
      "range:(2000, 4000) loss= 19.7326698303\n",
      "range:(4000, 6000) loss= 19.8441944122\n",
      "range:(6000, 8000) loss= 19.6927967072\n",
      "range:(8000, 10000) loss= 19.7455425262\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 86\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0795001984\n",
      "range:(2000, 4000) loss= 19.8325862885\n",
      "range:(4000, 6000) loss= 19.663608551\n",
      "range:(6000, 8000) loss= 19.8485946655\n",
      "range:(8000, 10000) loss= 19.7063407898\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7599754333\n",
      "range:(2000, 4000) loss= 19.8521842957\n",
      "range:(4000, 6000) loss= 19.8971004486\n",
      "range:(6000, 8000) loss= 19.936548233\n",
      "range:(8000, 10000) loss= 19.9214458466\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6802845001\n",
      "range:(2000, 4000) loss= 19.9743270874\n",
      "range:(4000, 6000) loss= 19.8945484161\n",
      "range:(6000, 8000) loss= 19.7470207214\n",
      "range:(8000, 10000) loss= 20.1155223846\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9304904938\n",
      "range:(2000, 4000) loss= 19.615606308\n",
      "range:(4000, 6000) loss= 19.7704086304\n",
      "range:(6000, 8000) loss= 20.1155853271\n",
      "range:(8000, 10000) loss= 19.6563968658\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8573150635\n",
      "range:(2000, 4000) loss= 19.7318668365\n",
      "range:(4000, 6000) loss= 19.8433761597\n",
      "range:(6000, 8000) loss= 19.6919937134\n",
      "range:(8000, 10000) loss= 19.7447223663\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 87\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0786724091\n",
      "range:(2000, 4000) loss= 19.8317832947\n",
      "range:(4000, 6000) loss= 19.6628093719\n",
      "range:(6000, 8000) loss= 19.8477878571\n",
      "range:(8000, 10000) loss= 19.7055034637\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7591247559\n",
      "range:(2000, 4000) loss= 19.851392746\n",
      "range:(4000, 6000) loss= 19.8962955475\n",
      "range:(6000, 8000) loss= 19.9357147217\n",
      "range:(8000, 10000) loss= 19.920633316\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6794757843\n",
      "range:(2000, 4000) loss= 19.9735298157\n",
      "range:(4000, 6000) loss= 19.893705368\n",
      "range:(6000, 8000) loss= 19.7462387085\n",
      "range:(8000, 10000) loss= 20.1147003174\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9297122955\n",
      "range:(2000, 4000) loss= 19.6147899628\n",
      "range:(4000, 6000) loss= 19.7695846558\n",
      "range:(6000, 8000) loss= 20.114736557\n",
      "range:(8000, 10000) loss= 19.6555938721\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8564796448\n",
      "range:(2000, 4000) loss= 19.7310466766\n",
      "range:(4000, 6000) loss= 19.8425521851\n",
      "range:(6000, 8000) loss= 19.691160202\n",
      "range:(8000, 10000) loss= 19.7439289093\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 88\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0778274536\n",
      "range:(2000, 4000) loss= 19.8309764862\n",
      "range:(4000, 6000) loss= 19.6620101929\n",
      "range:(6000, 8000) loss= 19.8469409943\n",
      "range:(8000, 10000) loss= 19.7046508789\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7582683563\n",
      "range:(2000, 4000) loss= 19.8505649567\n",
      "range:(4000, 6000) loss= 19.8954925537\n",
      "range:(6000, 8000) loss= 19.9348945618\n",
      "range:(8000, 10000) loss= 19.9198322296\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6786804199\n",
      "range:(2000, 4000) loss= 19.972738266\n",
      "range:(4000, 6000) loss= 19.8928833008\n",
      "range:(6000, 8000) loss= 19.7454051971\n",
      "range:(8000, 10000) loss= 20.1138916016\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9288482666\n",
      "range:(2000, 4000) loss= 19.6140041351\n",
      "range:(4000, 6000) loss= 19.768743515\n",
      "range:(6000, 8000) loss= 20.1138935089\n",
      "range:(8000, 10000) loss= 19.6547660828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8556346893\n",
      "range:(2000, 4000) loss= 19.7301883698\n",
      "range:(4000, 6000) loss= 19.841709137\n",
      "range:(6000, 8000) loss= 19.6903419495\n",
      "range:(8000, 10000) loss= 19.7431297302\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 89\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.07695961\n",
      "range:(2000, 4000) loss= 19.8301544189\n",
      "range:(4000, 6000) loss= 19.6611843109\n",
      "range:(6000, 8000) loss= 19.8461151123\n",
      "range:(8000, 10000) loss= 19.7037887573\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7574291229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(2000, 4000) loss= 19.8497200012\n",
      "range:(4000, 6000) loss= 19.8946437836\n",
      "range:(6000, 8000) loss= 19.9341163635\n",
      "range:(8000, 10000) loss= 19.9189910889\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.677854538\n",
      "range:(2000, 4000) loss= 19.9719104767\n",
      "range:(4000, 6000) loss= 19.8920860291\n",
      "range:(6000, 8000) loss= 19.7445926666\n",
      "range:(8000, 10000) loss= 20.1130428314\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9279785156\n",
      "range:(2000, 4000) loss= 19.6132049561\n",
      "range:(4000, 6000) loss= 19.7678699493\n",
      "range:(6000, 8000) loss= 20.1130104065\n",
      "range:(8000, 10000) loss= 19.6539077759\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8547992706\n",
      "range:(2000, 4000) loss= 19.7293262482\n",
      "range:(4000, 6000) loss= 19.8408870697\n",
      "range:(6000, 8000) loss= 19.6895084381\n",
      "range:(8000, 10000) loss= 19.7422981262\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 90\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0761241913\n",
      "range:(2000, 4000) loss= 19.8293132782\n",
      "range:(4000, 6000) loss= 19.6603279114\n",
      "range:(6000, 8000) loss= 19.845243454\n",
      "range:(8000, 10000) loss= 19.7029438019\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7565670013\n",
      "range:(2000, 4000) loss= 19.8488941193\n",
      "range:(4000, 6000) loss= 19.8938121796\n",
      "range:(6000, 8000) loss= 19.9332427979\n",
      "range:(8000, 10000) loss= 19.9181404114\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6770381927\n",
      "range:(2000, 4000) loss= 19.9710941315\n",
      "range:(4000, 6000) loss= 19.8912277222\n",
      "range:(6000, 8000) loss= 19.7437419891\n",
      "range:(8000, 10000) loss= 20.1121921539\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9271373749\n",
      "range:(2000, 4000) loss= 19.6124076843\n",
      "range:(4000, 6000) loss= 19.7669868469\n",
      "range:(6000, 8000) loss= 20.1121425629\n",
      "range:(8000, 10000) loss= 19.6531028748\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8539657593\n",
      "range:(2000, 4000) loss= 19.728471756\n",
      "range:(4000, 6000) loss= 19.8400363922\n",
      "range:(6000, 8000) loss= 19.6886405945\n",
      "range:(8000, 10000) loss= 19.7414608002\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 91\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0752468109\n",
      "range:(2000, 4000) loss= 19.8284702301\n",
      "range:(4000, 6000) loss= 19.6595211029\n",
      "range:(6000, 8000) loss= 19.8443698883\n",
      "range:(8000, 10000) loss= 19.7020778656\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7556915283\n",
      "range:(2000, 4000) loss= 19.8480644226\n",
      "range:(4000, 6000) loss= 19.8929653168\n",
      "range:(6000, 8000) loss= 19.9324111938\n",
      "range:(8000, 10000) loss= 19.9172725677\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6762294769\n",
      "range:(2000, 4000) loss= 19.9702720642\n",
      "range:(4000, 6000) loss= 19.890378952\n",
      "range:(6000, 8000) loss= 19.7428646088\n",
      "range:(8000, 10000) loss= 20.111328125\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9262523651\n",
      "range:(2000, 4000) loss= 19.6115722656\n",
      "range:(4000, 6000) loss= 19.7660961151\n",
      "range:(6000, 8000) loss= 20.1112575531\n",
      "range:(8000, 10000) loss= 19.6522808075\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.853105545\n",
      "range:(2000, 4000) loss= 19.7276229858\n",
      "range:(4000, 6000) loss= 19.8391971588\n",
      "range:(6000, 8000) loss= 19.6878051758\n",
      "range:(8000, 10000) loss= 19.7405967712\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 92\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0743293762\n",
      "range:(2000, 4000) loss= 19.827665329\n",
      "range:(4000, 6000) loss= 19.6586723328\n",
      "range:(6000, 8000) loss= 19.8435115814\n",
      "range:(8000, 10000) loss= 19.7012252808\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7548103333\n",
      "range:(2000, 4000) loss= 19.8471698761\n",
      "range:(4000, 6000) loss= 19.892124176\n",
      "range:(6000, 8000) loss= 19.9315395355\n",
      "range:(8000, 10000) loss= 19.9164237976\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6753807068\n",
      "range:(2000, 4000) loss= 19.9694385529\n",
      "range:(4000, 6000) loss= 19.8894748688\n",
      "range:(6000, 8000) loss= 19.7420082092\n",
      "range:(8000, 10000) loss= 20.1104488373\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9253864288\n",
      "range:(2000, 4000) loss= 19.6107234955\n",
      "range:(4000, 6000) loss= 19.7652397156\n",
      "range:(6000, 8000) loss= 20.1103553772\n",
      "range:(8000, 10000) loss= 19.6514167786\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8522338867\n",
      "range:(2000, 4000) loss= 19.7267227173\n",
      "range:(4000, 6000) loss= 19.8383483887\n",
      "range:(6000, 8000) loss= 19.6869621277\n",
      "range:(8000, 10000) loss= 19.7397251129\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 93\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0734596252\n",
      "range:(2000, 4000) loss= 19.8268070221\n",
      "range:(4000, 6000) loss= 19.6578063965\n",
      "range:(6000, 8000) loss= 19.8426265717\n",
      "range:(8000, 10000) loss= 19.700345993\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7539176941\n",
      "range:(2000, 4000) loss= 19.8463020325\n",
      "range:(4000, 6000) loss= 19.891248703\n",
      "range:(6000, 8000) loss= 19.9306850433\n",
      "range:(8000, 10000) loss= 19.9155464172\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6745700836\n",
      "range:(2000, 4000) loss= 19.9685916901\n",
      "range:(4000, 6000) loss= 19.8885669708\n",
      "range:(6000, 8000) loss= 19.7411327362\n",
      "range:(8000, 10000) loss= 20.1095905304\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9245128632\n",
      "range:(2000, 4000) loss= 19.6098461151\n",
      "range:(4000, 6000) loss= 19.7643260956\n",
      "range:(6000, 8000) loss= 20.1094646454\n",
      "range:(8000, 10000) loss= 19.65051651\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8513793945\n",
      "range:(2000, 4000) loss= 19.7258491516\n",
      "range:(4000, 6000) loss= 19.8374767303\n",
      "range:(6000, 8000) loss= 19.6860466003\n",
      "range:(8000, 10000) loss= 19.7388725281\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 94\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0726146698\n",
      "range:(2000, 4000) loss= 19.8259449005\n",
      "range:(4000, 6000) loss= 19.6569576263\n",
      "range:(6000, 8000) loss= 19.8417549133\n",
      "range:(8000, 10000) loss= 19.6994361877\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7530002594\n",
      "range:(2000, 4000) loss= 19.8453960419\n",
      "range:(4000, 6000) loss= 19.8903636932\n",
      "range:(6000, 8000) loss= 19.9297866821\n",
      "range:(8000, 10000) loss= 19.9146938324\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6737270355\n",
      "range:(2000, 4000) loss= 19.9677047729\n",
      "range:(4000, 6000) loss= 19.8876743317\n",
      "range:(6000, 8000) loss= 19.7402248383\n",
      "range:(8000, 10000) loss= 20.1086921692\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.923625946\n",
      "range:(2000, 4000) loss= 19.6089954376\n",
      "range:(4000, 6000) loss= 19.7634487152\n",
      "range:(6000, 8000) loss= 20.1085758209\n",
      "range:(8000, 10000) loss= 19.6496257782\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8505058289\n",
      "range:(2000, 4000) loss= 19.7249889374\n",
      "range:(4000, 6000) loss= 19.8366165161\n",
      "range:(6000, 8000) loss= 19.6851921082\n",
      "range:(8000, 10000) loss= 19.7379989624\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 95\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.071680069\n",
      "range:(2000, 4000) loss= 19.8250274658\n",
      "range:(4000, 6000) loss= 19.6560707092\n",
      "range:(6000, 8000) loss= 19.8408432007\n",
      "range:(8000, 10000) loss= 19.6985492706\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7520694733\n",
      "range:(2000, 4000) loss= 19.8444957733\n",
      "range:(4000, 6000) loss= 19.8894577026\n",
      "range:(6000, 8000) loss= 19.9288845062\n",
      "range:(8000, 10000) loss= 19.9138031006\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6728343964\n",
      "range:(2000, 4000) loss= 19.9668312073\n",
      "range:(4000, 6000) loss= 19.8867759705\n",
      "range:(6000, 8000) loss= 19.7393493652\n",
      "range:(8000, 10000) loss= 20.1078090668\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9227581024\n",
      "range:(2000, 4000) loss= 19.6081104279\n",
      "range:(4000, 6000) loss= 19.7625389099\n",
      "range:(6000, 8000) loss= 20.1076622009\n",
      "range:(8000, 10000) loss= 19.648771286\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8496608734\n",
      "range:(2000, 4000) loss= 19.7240905762\n",
      "range:(4000, 6000) loss= 19.8356990814\n",
      "range:(6000, 8000) loss= 19.6843090057\n",
      "range:(8000, 10000) loss= 19.7371520996\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 96\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0707588196\n",
      "range:(2000, 4000) loss= 19.8240833282\n",
      "range:(4000, 6000) loss= 19.6551780701\n",
      "range:(6000, 8000) loss= 19.8399219513\n",
      "range:(8000, 10000) loss= 19.6976032257\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7511425018\n",
      "range:(2000, 4000) loss= 19.8435611725\n",
      "range:(4000, 6000) loss= 19.8885478973\n",
      "range:(6000, 8000) loss= 19.927942276\n",
      "range:(8000, 10000) loss= 19.9129180908\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6719150543\n",
      "range:(2000, 4000) loss= 19.9659500122\n",
      "range:(4000, 6000) loss= 19.8858451843\n",
      "range:(6000, 8000) loss= 19.7384929657\n",
      "range:(8000, 10000) loss= 20.1068840027\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9218521118\n",
      "range:(2000, 4000) loss= 19.6071777344\n",
      "range:(4000, 6000) loss= 19.7616081238\n",
      "range:(6000, 8000) loss= 20.1067466736\n",
      "range:(8000, 10000) loss= 19.6479129791\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8487300873\n",
      "range:(2000, 4000) loss= 19.7231807709\n",
      "range:(4000, 6000) loss= 19.8348274231\n",
      "range:(6000, 8000) loss= 19.683391571\n",
      "range:(8000, 10000) loss= 19.736246109\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 97\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0698413849\n",
      "range:(2000, 4000) loss= 19.8231468201\n",
      "range:(4000, 6000) loss= 19.6542778015\n",
      "range:(6000, 8000) loss= 19.8389759064\n",
      "range:(8000, 10000) loss= 19.6966876984\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7502040863\n",
      "range:(2000, 4000) loss= 19.8426074982\n",
      "range:(4000, 6000) loss= 19.887638092\n",
      "range:(6000, 8000) loss= 19.9270267487\n",
      "range:(8000, 10000) loss= 19.9120025635\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.670999527\n",
      "range:(2000, 4000) loss= 19.9650497437\n",
      "range:(4000, 6000) loss= 19.8849220276\n",
      "range:(6000, 8000) loss= 19.7375888824\n",
      "range:(8000, 10000) loss= 20.1059551239\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9209003448\n",
      "range:(2000, 4000) loss= 19.6063098907\n",
      "range:(4000, 6000) loss= 19.7606887817\n",
      "range:(6000, 8000) loss= 20.1058044434\n",
      "range:(8000, 10000) loss= 19.6470031738\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8477859497\n",
      "range:(2000, 4000) loss= 19.7222614288\n",
      "range:(4000, 6000) loss= 19.8339233398\n",
      "range:(6000, 8000) loss= 19.6824245453\n",
      "range:(8000, 10000) loss= 19.7353210449\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 98\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0689334869\n",
      "range:(2000, 4000) loss= 19.8221988678\n",
      "range:(4000, 6000) loss= 19.6533508301\n",
      "range:(6000, 8000) loss= 19.8380260468\n",
      "range:(8000, 10000) loss= 19.6957435608\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.749288559\n",
      "range:(2000, 4000) loss= 19.8416500092\n",
      "range:(4000, 6000) loss= 19.8866996765\n",
      "range:(6000, 8000) loss= 19.9260978699\n",
      "range:(8000, 10000) loss= 19.9111042023\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6700706482\n",
      "range:(2000, 4000) loss= 19.9641551971\n",
      "range:(4000, 6000) loss= 19.8839797974\n",
      "range:(6000, 8000) loss= 19.7366600037\n",
      "range:(8000, 10000) loss= 20.105009079\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9199371338\n",
      "range:(2000, 4000) loss= 19.6053943634\n",
      "range:(4000, 6000) loss= 19.759759903\n",
      "range:(6000, 8000) loss= 20.1048870087\n",
      "range:(8000, 10000) loss= 19.6461219788\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8468532562\n",
      "range:(2000, 4000) loss= 19.7213306427\n",
      "range:(4000, 6000) loss= 19.8329868317\n",
      "range:(6000, 8000) loss= 19.681476593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:(8000, 10000) loss= 19.7344074249\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 99\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0679912567\n",
      "range:(2000, 4000) loss= 19.8212471008\n",
      "range:(4000, 6000) loss= 19.6524085999\n",
      "range:(6000, 8000) loss= 19.8370990753\n",
      "range:(8000, 10000) loss= 19.6948070526\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7482948303\n",
      "range:(2000, 4000) loss= 19.840713501\n",
      "range:(4000, 6000) loss= 19.8857707977\n",
      "range:(6000, 8000) loss= 19.9251232147\n",
      "range:(8000, 10000) loss= 19.9102096558\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6691818237\n",
      "range:(2000, 4000) loss= 19.9632205963\n",
      "range:(4000, 6000) loss= 19.8830451965\n",
      "range:(6000, 8000) loss= 19.7356910706\n",
      "range:(8000, 10000) loss= 20.1040363312\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9189929962\n",
      "range:(2000, 4000) loss= 19.6044845581\n",
      "range:(4000, 6000) loss= 19.7588062286\n",
      "range:(6000, 8000) loss= 20.1039333344\n",
      "range:(8000, 10000) loss= 19.6452026367\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.845949173\n",
      "range:(2000, 4000) loss= 19.7203979492\n",
      "range:(4000, 6000) loss= 19.8320808411\n",
      "range:(6000, 8000) loss= 19.6805038452\n",
      "range:(8000, 10000) loss= 19.7334861755\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 100\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 2000) loss= 20.0670318604\n",
      "range:(2000, 4000) loss= 19.8203392029\n",
      "range:(4000, 6000) loss= 19.6514511108\n",
      "range:(6000, 8000) loss= 19.8361434937\n",
      "range:(8000, 10000) loss= 19.6938610077\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 2000) loss= 19.7473220825\n",
      "range:(2000, 4000) loss= 19.8397369385\n",
      "range:(4000, 6000) loss= 19.8848190308\n",
      "range:(6000, 8000) loss= 19.9241924286\n",
      "range:(8000, 10000) loss= 19.9092884064\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 2000) loss= 19.6682510376\n",
      "range:(2000, 4000) loss= 19.9622840881\n",
      "range:(4000, 6000) loss= 19.8821334839\n",
      "range:(6000, 8000) loss= 19.7347640991\n",
      "range:(8000, 10000) loss= 20.1030807495\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 2000) loss= 19.9180107117\n",
      "range:(2000, 4000) loss= 19.6035518646\n",
      "range:(4000, 6000) loss= 19.7578315735\n",
      "range:(6000, 8000) loss= 20.1029701233\n",
      "range:(8000, 10000) loss= 19.6442241669\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 2000) loss= 19.8449935913\n",
      "range:(2000, 4000) loss= 19.7194271088\n",
      "range:(4000, 6000) loss= 19.8311500549\n",
      "range:(6000, 8000) loss= 19.6795692444\n",
      "range:(8000, 10000) loss= 19.7325801849\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell. Since, the data used for this task is CIFAR-10, \n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "    \n",
    "    It took me around 5hrs to execute this cell entirely.\n",
    "'''\n",
    "\n",
    "with tf.Session(graph=computation_graph) as sess:\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "         # load the weights from the model1\n",
    "        saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    else:\n",
    "        # create a new saver\n",
    "        saver = tf.train.Saver(max_to_keep=2)\n",
    "        \n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for ep in range(50, no_of_epochs):  # epochs loop\n",
    "        \n",
    "        print \"epoch: \" + str(ep + 1)\n",
    "        print \"=================================================================================================\"\n",
    "        print \"=================================================================================================\"\n",
    "        \n",
    "        for batch_n in range(no_of_batches):  # batches loop\n",
    "            \n",
    "            # retrieve the operations from the graph to be evaluated\n",
    "            loss = sess.graph.get_tensor_by_name(\"loss:0\")\n",
    "            train_op = sess.graph.get_operation_by_name(\"train_op\")\n",
    "            inputs = sess.graph.get_tensor_by_name(\"inputs:0\")\n",
    "            \n",
    "            # generate the batch images and labels\n",
    "            batch_images, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_\" + str(batch_n + 1)))\n",
    "            \n",
    "            min_batch_size = 2000 # we look at only 500 images at a time since the machine is small\n",
    "            \n",
    "            print \"current_batch: \" + str(batch_n + 1)\n",
    "            \n",
    "            for index in range(len(batch_images) / min_batch_size):\n",
    "                start = index * min_batch_size\n",
    "                end = start + min_batch_size\n",
    "                _, cost = sess.run([train_op, loss], feed_dict={inputs: batch_images[start: end]})\n",
    "                print('range:{} loss= {}'.format((start, end), cost))\n",
    "            \n",
    "            print \"\\n=========================================================================================\\n\"\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0):\n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(model_path, \"model1\"), global_step = (ep + 1))\n",
    "        \n",
    "    print \"=================================================================================================\"\n",
    "    print \"=================================================================================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's visualize the representation of a random image and it's reconstructed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Models/Model1/model1-100\n",
      "[[ 99  94 102 100  98  92 109 101 101 105 101 100]\n",
      " [110 104 108  99  64  74  97  97 115 115 112 112]\n",
      " [103  78  47  59  36  40  66  62  92  97  97 100]\n",
      " [ 87  45  33  30  39  38 121  79  58  89  90  94]\n",
      " [ 75  30  40  23  50  89 215 158  96 112  82  87]\n",
      " [ 79  56  29  22  38 149 185  86 123 182 118  85]\n",
      " [ 34  38  27  31  29 167 181  46 108 202 203 110]\n",
      " [ 25  20  18  24 100 152 103  48 119 196 230 180]\n",
      " [ 22  22  17  21 149 116  28  29 102 185 212 237]\n",
      " [ 22  20  13  21  99  86  28  16  63 162 202 228]\n",
      " [ 20  14   7   8  43  57  18  12  56 133 170 220]\n",
      " [ 34  11  14  10  15  22  16  12  40  95 155 217]]\n",
      "[[ 110.25523376  113.2928772   111.25167847   99.01824188   92.04980469\n",
      "    87.7914505    89.18729401   93.07136536   95.82571411  102.33727264\n",
      "   106.20143127  110.0054245 ]\n",
      " [ 110.00231171  105.68144226   99.81483459   89.9105072    79.42810822\n",
      "    75.88776398   80.15633392   81.71257019   92.08976746   94.98019409\n",
      "   100.47554779  106.35533905]\n",
      " [  99.05879974   95.46566772   81.32810974   69.61352539   59.81778336\n",
      "    66.77558136   69.0868988    81.97225952   90.07959747   94.80982971\n",
      "    96.42903137  100.25978088]\n",
      " [  83.71749115   73.36076355   64.41521454   57.11665344   52.8359375\n",
      "    58.62495804   78.21392822   92.74477386   96.72573853   97.85636902\n",
      "    90.69501495   87.90641022]\n",
      " [  64.70111084   60.28451157   52.07117844   52.36153412   57.86801147\n",
      "    76.65405273   97.14846802  114.76940155  116.17497253  114.77095032\n",
      "   105.99909973   93.20736694]\n",
      " [  50.03802109   47.88028336   42.29443741   51.20830154   64.97366333\n",
      "    93.06890869  122.7744751   138.53744507  147.03414917  137.05059814\n",
      "   126.89653015  114.30675507]\n",
      " [  40.74979782   41.48952484   37.63527298   52.70973587   65.70045471\n",
      "    96.41921234  121.9504776   141.31877136  148.43711853  156.15550232\n",
      "   150.73963928  134.87471008]\n",
      " [  32.1354866    25.82136726   35.50828934   45.21402359   66.19862366\n",
      "    85.85202789  101.03109741  118.80303192  128.39916992  151.36245728\n",
      "   157.2819519   156.02210999]\n",
      " [  26.01289749   28.9689579    32.96279526   45.71814728   58.78302002\n",
      "    63.75247192   69.73912811   81.45835876  101.15275574  127.87757874\n",
      "   150.66107178  166.42384338]\n",
      " [  22.36808395   25.99650955   36.09705353   46.35605621   44.24160385\n",
      "    44.1945343    37.47689438   53.14644241   78.80060577  121.02045441\n",
      "   165.57925415  181.56295776]\n",
      " [  24.68787193   27.75665283   35.28474808   42.75697327   43.61241531\n",
      "    34.03214645   23.958395     25.81012726   61.65138626  123.79983521\n",
      "   173.11846924  194.12667847]\n",
      " [  11.61035538   15.6897068    21.89349937   35.64909363   35.03063202\n",
      "    20.09486771    6.62233353   11.91489029   47.9420433   108.02458954\n",
      "   174.27166748  211.53198242]]\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXmQ3Vd157/nLf1619aWaMuy5EW2sfF4iTA4OAzDFuOB\n2GSCwxJiahzMEKBgikwKnAqYqkwGqABFMhMYAa44mMEwxsZOcIGNMXhYLFvyIlmWJVubtXZbS0ut\nXt565o/3FFry/d5+UqtfS77fT5VKr+959/e7v/t+5/f7vft95xxzdwgh0iMz0wMQQswMcn4hEkXO\nL0SiyPmFSBQ5vxCJIucXIlHk/CchZnazmX3zRL+3iW25mZ17IrYlTn5MOv/0YmYfAPBJAOcAOAjg\nbgCfdvehmRxXCDNzAEvd/fmA7ecAbnf3E3KhETOP7vzTiJl9EsAXAPw3ALMAvBbAYgAPmFkb6ZNr\n3QhFysj5pwkz6wXwOQAfc/cfu3vZ3bcAuB7AEgB/0njfLWZ2p5ndbmYHAXyg0Xb7hG39qZltNbO9\nZvbXZrbFzN48of/tjddLGo/uN5jZC2a2x8z+asJ2rjCz35jZkJntMrP/yS5CkxzbG8xsu5n9pZkN\nNrZ1nZldY2YbzGyfmd3c7H7N7K1mtt7MDpjZP5rZL8zszybY/7OZrTOz/Wb2EzNbfKxjFi9Fzj99\n/C6AdgB3TWx090MA7gPwlgnN1wK4E8BsAN+Z+H4zuxDAPwJ4H4B+1J8gFk6y76sAnA/gTQA+Y2av\nbLRXAfxXAH0ArmzY//wYj+swr0D9+BYC+AyAb6B+QfsdAL8H4K/N7KzJ9mtmfagf+6cBzAOwHvW5\nQ8N+LYCbAfwhgNMA/D8A3z3OMYsJyPmnjz4Ae9y9ErDtatgP8xt3/6G719x97Kj3/hGAf3H3X7p7\nCXVHm2yh5nPuPubuTwF4CsAlAODuq9z9EXevNJ5C/jeAf3/shwYAKAP47+5eBnBH43i+6u7D7r4W\nwDNN7vcaAGvd/a7GXP09gN0T9vNfAPwPd1/XsP8tgEt19586cv7pYw+APvIdvr9hP8y2yHZOn2h3\n91EAeyfZ90TnGQXQDQBmdp6Z/auZ7W58xfhbHHkROhb2unu18frwBWtggn2syf0efXwOYPuE7SwG\n8NXGV4YhAPsAGCZ/+hGTIOefPn4DoIj64+q/YWbdAN4G4MEJzbE7+S4AZ0zo34H64/Hx8DUAz6K+\not+L+uO0Hee2TtR+jz4+m/g36heGD7n77An/Otz91y0Y98saOf804e4HUF/w+wczu9rM8ma2BMD3\nUb+zfbvJTd0J4B1m9ruNRbJbcPwO24O63HjIzC4A8OHj3M6J3O+PAFzcWDDMAfgI6usJh/k6gE+b\n2UUAYGazzOxdLRr3yxo5/zTi7l9E/S73d6if/CtQv5O9yd2LTW5jLYCPof69eheAQwAGUX+qOFb+\nAsB7AQyjvkD3vePYxvFA9+vuewC8C8AXUf86cyGAlWgcn7vfjbpcekfjK8PTqD85iSmiH/mcYjS+\nNgyh/gi9eabHc6IxswzqT0bvc/eHZno8L2d05z8FMLN3mFmnmXWh/hSxBsCWmR3VicPMft/MZptZ\nAb9dD3hkhof1skfOf2pwLYCdjX9LAbzbX16PbFcC2Ii6AvIOANcFJE9xgtFjvxCJoju/EIki5xci\nUeT8QiSKnF+IRJHzC5Eocn4hEkXOL0SiyPmFSBQ5vxCJIucXIlHk/EIkipxfiESR8wuRKHJ+IRJl\nStVhzOxqAF8FkAXwTXf/fOz97Z1d3jNrdtAWCyzmCet4L69FbNUqteXyWWqrsZFEBh9LthcNpo50\ntOiMhG3HG7ltkXFkIkYjtmyG328ipiixY2Mh67VIp/hc8WOuxbpFCe8wNo4M6XNwaB/GRkeayvF4\n3M5vZlkA/wv14hPbATxmZve6+zOsT8+s2bj2Ax8K2srGna6dTGvN+XTXRnkuiNKhQ9Q2Z3744gQA\n42yMZdoFucgHWMlETsCIJ+SQj/QLj7FY4fuKnbTtkYthIc/HUciET62erk7ap6uLn7OZyOlcLPKL\nebkcto2VIn0iE1KLVFOLbBKIfNaVWinY7jV+DhSq4T53fPMrkUEcNaSm3/lSrgDwvLtvahSTuAP1\njDNCiFOAqTj/QhxZbGI7VEhBiFOGaV/wM7ObzGylma0cGx2Z7t0JIZpkKs6/A8CiCX+f0Wg7Andf\n7u7L3H1ZR2fXFHYnhDiRTMX5HwOw1MzOalSSeTeAe0/MsIQQ081xr/a7e8XMPgrgJ6hLfbc2qstQ\nMvk8uvrDywKliK6RK44G28cODNM+m7by2pedWX7Ny3d3UBsK4ZXquX2n8z7G91WNXXsj/TIRZcSy\nbcH2QmwFm5uQiaxSF3L89OnIF4LtnZ18frs7+XF1tLdTW7nMx1ithI/uQGRJ/0AxvJIOALWIhNxO\nlBYAyOX551mqhuUir4YKPNdpKx0MtmcjCsxLxtT0OwO4+32o15oXQpxi6Bd+QiSKnF+IRJHzC5Eo\ncn4hEkXOL0SiTGm1/1ipuaNYCcsau3a/5PdB/8bWNU8G27uJnAQAAy9sp7ZcjUdgjBcPUFvFw/sr\nL+UST75nFrXBuCyTjdgsG4kWypIImExEAooEEdXA5aaxEo9oGs2EpbSDo1xGay/w6J32di4RWmSu\nWOzXSGQKxyMSG5teACi0cWN5OCzNAcBza58KthfLfH4vODssL1cjYz8a3fmFSBQ5vxCJIucXIlHk\n/EIkipxfiERp6Wp/tVzC0I7wKvzmZ8IrngBwaMfmYPv+Ml/Z7IisbtdK49T2mkveSG3PrH8h2L5p\n3dO0z+KLL6E2M57SKmc88CTXFlEJSACJ5bgiwfL+1Ttyk0eyEFYqYUXFIqvsh4hCAACjkZRt2Uw4\nmAkA4OH5qEbGno2oKbkaP+fGh3i+iq1PPkptv/rRD4Pt5RxXs0YvvzjcHlEVjkZ3fiESRc4vRKLI\n+YVIFDm/EIki5xciUeT8QiRKawN7qhUcOrgnaDuwb5D26ySy3f5RLmv0LeyntvMXn0dtf/qed1Lb\nb371WLD94RVcppzdzaf4hd08B2G+rYfaasYDkwok8iQbrSUVyyXIO3o0PyHTCCPaYTUisYEfsxuX\nbo3khuTjA/JVfsw7NqymtidX/JraqsND1GblcI7KbI0H9jz7+Kpg+/hoeFshdOcXIlHk/EIkipxf\niESR8wuRKHJ+IRJFzi9EokxJ6jOzLQCGAVQBVNx92aQ7JJFbp5/Opbkda9YH2925XHNo5BC1XfX6\nq6jthS3hCEIA+PG99wTbl15wIe3zuit5VN/PV4SPCwA279xPbcjzfHa1SnhOLFKUKybZRaY4mvuP\n2WI5AbORnHXI8qjEbBuPfmOqXSbLT/39A7uo7bGHHqC2HZuep7bOTh7BWcuEx1KL5OOrjocPzCNl\n747mROj8/8Hdw+K9EOKkRY/9QiTKVJ3fAdxvZqvM7KYTMSAhRGuY6mP/Ve6+w8zmA3jAzJ5194cn\nvqFxUbgJADq6u6e4OyHEiWJKd35339H4fxDA3QCuCLxnubsvc/dlbZEa60KI1nLczm9mXWbWc/g1\ngLcC4MnshBAnFVN57F8A4G4zO7yd/+PuP451GBsZwZrHwpFxuXwk4WY1LAF15HnixrF9+6htbheX\nXSolLok9u25jsH3+PC5T9nVyiepdb3s1tX37ez+itqFxLgHlckTqy3DNzonUBAAWSXSZj/TLkmjA\n9WvX0j47Nj5DbYWOLmo756LLqO20RWcH27ORyMi9O3mpt8FdO6ktE4kULI4Xqc1JxGJMtssQKbUl\nUp+7bwLARWwhxEmNpD4hEkXOL0SiyPmFSBQ5vxCJIucXIlFamsDTq1WM7QtHq5XLJdqvVgpLW20R\nqentb/49ajuweze1/Xztc9SWIdF0XRHpsBRJMnr2gnnU9tZINOCn/+bL1HbmuRcE22NyWC1yC8hG\n5KtdWzdR2/7BsCT2/Hou5w1u5xGVyHJZd9vWcA1FAPj9P/hPwfYFCxbQPsWDXCYuGJ+P8YjKVqtx\nY9W57Mhgkt6xSH268wuRKHJ+IRJFzi9Eosj5hUgUOb8QidLa1f5aDVVSTih2FaqSXGbVMl8lvez8\npdT2/BpeculH9/2C2qwQDi7ZPzZC+4xEyiftG+Sryldexlfnzzl9LrWteixcMur8iy6mffLkuABg\ny/M8L90jv/gpte3fvS28L+d5+nKRXIKIrGIfGuCBOE/87L5g+1nnhVURAFj35OPUVorkhsw4D+KK\nreezFXqLKAvHsqrP0J1fiESR8wuRKHJ+IRJFzi9Eosj5hUgUOb8QidJSqQ8OsKpRtRrPnVcph6W+\nXI7n/Xv80SepbXh/pBRWpD7VMJHt1j/HA1IGBg9Q25xuHtgzNj5Gbe979/XU9sTNnwu279wYzj8I\nAGXn94BHHwlLhwAwtHeA2pzkXaxEglhqkTyDQEQijNzCtm1eF2zftWMr7TM2wuc+GwnQqUWOLVb2\njCp6J0DOi6E7vxCJIucXIlHk/EIkipxfiESR8wuRKHJ+IRJlUqnPzG4F8HYAg+7+qkbbXADfA7AE\nwBYA17t7RD/7LTWEI59KFS6TlMphGbBc4RFzjzzOy0L1tvOce6NFXlapQpSX4iiXoba9sIva2vN8\n+ufO55F7V175Gmr74z+8Ntj+0MMraJ/BPUPUtn/fXmpzj8izVRapxiPfjOnAABDZFyLRgKys1Xgk\nOo9+0AAykRJxiMjV1UiOSssce1Qf3dYxvLeZO/8/Abj6qLZPAXjQ3ZcCeLDxtxDiFGJS53f3hwEc\nHXh+LYDbGq9vA3DdCR6XEGKaOd7v/Avc/fDz7G7UK/YKIU4hpvzzXnd3M6NfkszsJgA3Afz7lxCi\n9RyvNw6YWT8ANP4fZG909+Xuvszdl2ViaZqEEC3leL3xXgA3NF7fAOCeEzMcIUSraEbq+y6ANwDo\nM7PtAD4L4PMAvm9mNwLYCoCHmU0gk82it7c3aBsY4BFihUIh2F5BONoPALYP7eHbi5T5qrVxKYpJ\nQENDPHJvw4b11Da/r4faqhUuDXV2cEHn4x//YLC9Ar6922+/k9qiT2sR2a7K5LJIoFpM2fJI5KE5\nj+6sERW2VuXSckwuq1T4ORdNqnnsql2UbJbMPR/eS5jU+d39PcT0puZ3I4Q42dCXcCESRc4vRKLI\n+YVIFDm/EIki5xciUVqawNPMkMuFd0mlCwBtJJKqXObRdNUc11bGI3qTH0ck1eAglykHB19BbRb5\nxeOGDc9SW3dPO7XNmz8/2P7xj4UlQAAYHeW1Bn/yY57A8+BBHhlXrIajIy2WpDOSHDMTkRWzER2t\nQrZpkfueO9fLYolmY3pe9LjJ+ch8BVCtPiHEFJDzC5Eocn4hEkXOL0SiyPmFSBQ5vxCJ0lKpr1Kp\n4MUXXwzaqjUeZTVeHA+2W0R2idVGK7NQLwDZWiTBZI1cKyORb319fdTGJEwAWLzoLGqrlbjMs2vb\nzmB7riMcGQkAH/nwjdT2qldeTG1rnwnXwQOAzZs3BdvHx3gdvJHRg9R24ABP1jo4wHPHVorh88oy\nkc85JsvFFLaomsc7sv3FZMVaxF+aRXd+IRJFzi9Eosj5hUgUOb8QiSLnFyJRWrra7+4olcIr7ZUy\nD6bIZ0mOtkgetioiq6GRVdm2Nj4l3R3hMl9GSpABQP/ChdR2+sLTqW3e7DnUtnfw6Boqv6VMsqjX\nMrwk17w+vq+rXncZtc0/LZyPEQAWnDY72P7ii7z81+WXc2VhwSt4aYiNm7ZQ2wP3/yzYvuLRp2if\noQM80CkTUwli99JIwJgTaSoSX4QqOYmPJd5Hd34hEkXOL0SiyPmFSBQ5vxCJIucXIlHk/EIkSjPl\num4F8HYAg+7+qkbbLQA+COBwlM7N7n7fZNvKZICeLnK9qcRytIWHWa7xYJWq8/JUluEBE3Pn8vx4\nvd1h2yiPOcHuPVxiW79hA7UtmjuL2sqjXMbcsO2FYHv/wnm0z9ln9lNb35xuatuzeze1dbZ1Bds3\nbohIbC9yiW3hIi5H3vjn76W2P37vm4PtDz30GO3zwP2/obZ/uZef5iMj4byFAOCR+2ytErbx2tdA\nzcLnsEcjj46kmTv/PwG4OtD+FXe/tPFvUscXQpxcTOr87v4wAP6rEiHEKclUvvN/1MxWm9mtZsaf\nyYQQJyXH6/xfA3AOgEsB7ALwJfZGM7vJzFaa2cpaJC+7EKK1HJfzu/uAu1fdvQbgGwCuiLx3ubsv\nc/dlmWjhAiFEKzku5zezicvD7wTw9IkZjhCiVTQj9X0XwBsA9JnZdgCfBfAGM7sU9YxmWwB8qJmd\nFQpZnHNuT9iW59ehTDU8zApX81CNRPxlspF8ahF9pURy5+3cz3PPPfrISmq79PxzqK1W4ePYu2eQ\n2kZGwnLZ+BiX7GplPlee4xJsoY1LrYW2cH7C0RGui27ZvJ3aHv4lz9M3XuLb/MubPxZsv/Y/hgSs\nOlctu5La+ufyebzrB/dQW7QMnIdlO6tySXqsGg7527k34hRHManzu/t7As3fanoPQoiTEv3CT4hE\nkfMLkShyfiESRc4vRKLI+YVIlJYm8MyYoZALJ+PsbOfXoRxJkJnr4HJYJSKVWTSZIpdXRki3TKTP\n6PAhasvU+PRXSlx+W73mCWo7UAqPJZak88AQH2OhxOW8zk4ue/X0hm0jowdon7Eyj4ob5VW+8MRj\nz1HbM09sDLb/3uvCCUYBYF5vOCIRAK6/7hpqe/YpHik4PMLl4O7O8HmQAz+vKhb2o6GVm2mfo9Gd\nX4hEkfMLkShyfiESRc4vRKLI+YVIFDm/EInSUqkPMGQ8LFGUIlkwsyR6rJblxczKNa4NGUjtPwDl\nWIQbUQ+7Orl0mM2Eo9sAoFIep7an1zxJbWPjPNHlnn1h2+bNW2mfvjnhSEsA6J3NbT3dXOo7//yl\nwfZ5pIYfAOzYu4faPMPvU7t38fp/I0TGrJb4uVMhEXMAcObiRdS2+Cxel3HV47uoraujI9ieyfJz\nkZ1Xx5IxQ3d+IRJFzi9Eosj5hUgUOb8QiSLnFyJRWrran8sa5swO77JW5v26O8PXqNFIujKr8txz\n7vyaZxEbG8eC+XyNtatrPrX1dnPV4Wc/u5/azlp6AbWde244L+D+/TwH3ooVPM/gwkV8BbuzmysB\ne/aF9zc+zlWYco0H9mTzndTG8hYCwLatYZVjbJwmnEYpkv9x1mw+jvMuuojaVqxaRW2VKjnnPKJm\neXiMNSZJBdCdX4hEkfMLkShyfiESRc4vRKLI+YVIFDm/EInSTLmuRQD+GcAC1MtzLXf3r5rZXADf\nA7AE9ZJd17s715NQl/oWzAkHJOSqfbSfIyxf9BTaaZ9inmuHsWrBHrFlsmH50Et8Guf3z+O2+Vwq\nGykOU9uKJ7g019YelqJGI0nwxse4bdYs/rl0RQJ79hGpb9umLZFxcO02l+GfyxkLuRw5PBIOntoz\nwHPqdXZxCbbWyQO1zltyPrV1RKTKIjnsfAc/vz1SVq5ZmrnzVwB80t0vBPBaAB8xswsBfArAg+6+\nFMCDjb+FEKcIkzq/u+9y98cbr4cBrAOwEMC1AG5rvO02ANdN1yCFECeeY/rOb2ZLAFwGYAWABe5+\nOEh5N+pfC4QQpwhNO7+ZdQP4AYBPuPsRX5jc3YHwF3Mzu8nMVprZyrEi/7miEKK1NOX8ZpZH3fG/\n4+53NZoHzKy/Ye8HECwa7+7L3X2Zuy/rKLQ4cZAQgjKp81u9vM23AKxz9y9PMN0L4IbG6xsA3HPi\nhyeEmC6auRW/DsD7Aawxs8OJ5W4G8HkA3zezGwFsBXD9ZBsyAFkSdVQZ55FURVLGqdDGh5+JRGah\nxssgZSO54mhEWo1H9fXPP43axka53JTLRfK3lbiMOX4wvM22HI9yzLbxfZXHtlFb0bkUxdSyi87l\nOfxm7+M5DXfu3kdt1QqXKvePhG3PbeQ5DZecweXZ7kgk5hn9Z1DbaX1cMt25c3uwvTdSDi1WIq5Z\nJnV+d/8leF7AN015BEKIGUG/8BMiUeT8QiSKnF+IRJHzC5Eocn4hEqWlv7pxr6FYIkkaI8oci+qj\n2wJQq/CoJzN+zctGJLEC2eTsnnA5MQBYeu7Z1JbPcrkmV+MRbnO5woZqW/jYMhkuR9aq3GYZfork\ncpEISCKZziGlqQCgr5dHvi2a20Vt2/fwBJ73/zScCHXfwADt86E/+xNq6+zi4xgceJHaFi3iMuDQ\n3vBYMhGnyBCfID+0JdsQQiSJnF+IRJHzC5Eocn4hEkXOL0SiyPmFSJSTJsC+vcDlMsuF5aHi2Cjt\n09bOt5fJRmr1GZe9KpmwtJjN83294jQeITa4Ywu1eYXLmPlIXoRqKRwZV41IQLkcT0rZluXyW6nI\nowtzbWHJNJ/nY2/L8+i8jj4uEXZ2c/ltw5Zgmgk8tuLXtM/Vb3k9tb36ysuoraOTz1Xf3LnU1tke\nnv8MePKbjgLpE5F0X7p9IUSSyPmFSBQ5vxCJIucXIlHk/EIkSosDexyVcngFsxiJR8hUwyvHRVbn\nCEAuy1ewY5WOapHcf3kS9FMt8Q3WKpF8e2M8IKVa4yu9lRq/ZueJypElpcaA+Ap8JhJ8lI0EGLGg\nHzN+XOxzBoDSOP+sMxU+xiULeoPtB4b5Z3bPD++mtjlzuOqw7NVXUNvcuXOorb0tnBcwH7k1O1FT\nMhG16iXvbfqdQoiXFXJ+IRJFzi9Eosj5hUgUOb8QiSLnFyJRJpX6zGwRgH9GvQS3A1ju7l81s1sA\nfBDA4cRlN7v7fbFtteXzOLM/XMn7UJEHspBqXUCWD39kPBIYE5GbWMAEwMsWtUeksoFtW6ht74s7\nqa1rFg8SiX1qXR1h/a0zclxMwgSA4REePDUyzuexlA1LppUal9hGhrmcF62+FpFuiYqGrtm87NbQ\nQZ6Lb90Tq6jtvLMWUduZi15BbT1Mnq1xmdjawyfBsQT2NKPzVwB80t0fN7MeAKvM7IGG7Svu/ndN\n700IcdLQTK2+XQB2NV4Pm9k6AAune2BCiOnlmL7zm9kSAJcBWNFo+qiZrTazW82M/4RJCHHS0bTz\nm1k3gB8A+IS7HwTwNQDnALgU9SeDL5F+N5nZSjNbOTLGv8MIIVpLU85vZnnUHf877n4XALj7gLtX\n3b0G4BsAgj9sdvfl7r7M3Zd1dfBFFiFEa5nU+a2e1+pbANa5+5cntPdPeNs7ATx94ocnhJgumlnt\nfx2A9wNYY2ZPNtpuBvAeM7sUdflvC4APTbahTMbR2RGWhyptkdJPRIly43JeLrK9nHNbW4bLTaNj\nYb2pUOA55Aod/Pq6d4hLSrl2/pRUiNhYJFg2Eyv9xOUhr3FbtcznsVQKz6NFoguzOb6vAoliq/fj\np3GtFo74y1W4TJlzvr3edv55jg0PUdvpROIGgFk94fNnbIhvr+bk84yc20fTzGr/LxGWuKOavhDi\n5Ea/8BMiUeT8QiSKnF+IRJHzC5Eocn4hEqWlCTyrNcehsbAENMKVFxSy4ain3i6eQbK9wG0Z47JR\nkYwPAMaqYVvXPP7L5tPP4ZFev1z5K2obOsRlzJ6I/GYk8ad38pJiqPFjrlT5vrKRJKld5MwqRxKa\nthW4TFVo4+PIRzJduof75Y3LpZFgOgwNDfBxRMqodUbKx/X39wfbd4zvpX1KRDKNlZs7Gt35hUgU\nOb8QiSLnFyJR5PxCJIqcX4hEkfMLkSgtlfqK41U8t/5A0LZnlGt9s7q7g+3dbbxG23jxILVVI1Fb\nxSLfZpUkR/yD11xA++w+cIjaVj+3ldpqZT6O7k4uzXWQjJX7D/HtxTJgliKyYkckunBOezhSbc4s\nLg96lWtsZZrFFSiR+o8AYBa+v+UQiews8OPatXc3tT29djW1nXvOudS2+MzFwfY9OzbQPodK4fnw\nY4jq051fiESR8wuRKHJ+IRJFzi9Eosj5hUgUOb8QidJSqQ9mNIFjRIgCSJ9YFYAXh8epbWScyyGD\ne4ep7eJLfifYft5Fr6F9vvmtr1Pb1p1cjjSLROEZl/pYTT6vcKksJg7VIhGQbZGEm3N7w+Of09tB\n++RYplYAxUgtR5akEwA6O8P7687zfRU6uS3fyY95716ekDVb4wlUZ82eHWy3DL83V0nxQkl9QohJ\nkfMLkShyfiESRc4vRKLI+YVIlElX+82sHcDDAAqN99/p7p81s7MA3AFgHoBVAN7v7nwZGkCuLYs5\ni+cFbcXde2i/8lh4VbwYWUHN9fDr2vz5PdRWmMMDT3Zu3xhsX/73/0D7mI1S22svOYvaqpFgm5Fx\nvvLd1RnOXdjVwVfZ2coxABQjQTPF8RFqyyLcL5/l2+tq56vso2R7dfgKPFnsR0dkX+NlrhS9uIOf\np+MHuHrzyvPOo7Zll14YbJ/dzcdYzoWD3XLZ5u/nzbyzCOCN7n4J6uW4rzaz1wL4AoCvuPu5APYD\nuLHpvQohZpxJnd/rHI5LzTf+OYA3Ariz0X4bgOumZYRCiGmhqWcEM8s2KvQOAngAwEYAQ+5++Fls\nO4CF0zNEIcR00JTzu3vV3S8FcAaAKwDw7BVHYWY3mdlKM1s5Nh773iaEaCXHtNrv7kMAHgJwJYDZ\nZnZ4wfAMADtIn+Xuvszdl3W0t/bXxEIIzqTOb2anmdnsxusOAG8BsA71i8AfNd52A4B7pmuQQogT\nTzO34n4At5lZFvWLxffd/V/N7BkAd5jZ3wB4AsC3JttQDRmMWViK6j8jXLIIAA7tCUtbReNy2NDw\nELV1dYVlEgCYPYeX3ir1hpXM9iwPLDl7yZl8e+P7qC2b5fLVgUNcPmRhOt3dXN6MVXiqVCJSX5HP\nY4bkO+zt7aV9utr4vWj/Xl66qq2Ny7PtHaRsW6xkW4VPyLbtPHjnYKTm3MCBMWrbunVTsL1/IV9G\nm9sWPk/bf/YC7XM0kzq/u68GcFmgfRPq3/+FEKcg+oWfEIki5xciUeT8QiSKnF+IRJHzC5Eodiw5\nv6a8M7MXARyuUdUHgIdItQ6N40g0jiM51cax2N1Pa2aDLXX+I3ZsttLdl83IzjUOjUPj0GO/EKki\n5xciUWZAZXK1AAAC9UlEQVTS+ZfP4L4nonEcicZxJC/bcczYd34hxMyix34hEmVGnN/Mrjaz9Wb2\nvJl9aibG0BjHFjNbY2ZPmtnKFu73VjMbNLOnJ7TNNbMHzOy5xv88vHB6x3GLme1ozMmTZnZNC8ax\nyMweMrNnzGytmX280d7SOYmMo6VzYmbtZvaomT3VGMfnGu1nmdmKht98z8x4OGMzuHtL/wHIop4G\n7GwAbQCeAnBhq8fRGMsWAH0zsN/XA7gcwNMT2r4I4FON158C8IUZGsctAP6ixfPRD+DyxuseABsA\nXNjqOYmMo6Vzgno64u7G6zyAFQBeC+D7AN7daP86gA9PZT8zcee/AsDz7r7J66m+7wBw7QyMY8Zw\n94cBHB3Mfy3qiVCBFiVEJeNoOe6+y90fb7weRj1ZzEK0eE4i42gpXmfak+bOhPMvBLBtwt8zmfzT\nAdxvZqvM7KYZGsNhFrj7rsbr3QAWzOBYPmpmqxtfC6b968dEzGwJ6vkjVmAG5+SocQAtnpNWJM1N\nfcHvKne/HMDbAHzEzF4/0wMC6ld+xCtnTydfA3AO6jUadgH4Uqt2bGbdAH4A4BPufkQFjFbOSWAc\nLZ8Tn0LS3GaZCeffAWDRhL9p8s/pxt13NP4fBHA3ZjYz0YCZ9QNA4//BmRiEuw80TrwagG+gRXNi\nZnnUHe477n5Xo7nlcxIax0zNSWPfx5w0t1lmwvkfA7C0sXLZBuDdAO5t9SDMrMvMeg6/BvBWAE/H\ne00r96KeCBWYwYSoh52twTvRgjkxM0M9B+Q6d//yBFNL54SNo9Vz0rKkua1awTxqNfMa1FdSNwL4\nqxkaw9moKw1PAVjbynEA+C7qj49l1L+73Yh6zcMHATwH4KcA5s7QOL4NYA2A1ag7X38LxnEV6o/0\nqwE82fh3TavnJDKOls4JgH+HelLc1ahfaD4z4Zx9FMDzAP4vgMJU9qNf+AmRKKkv+AmRLHJ+IRJF\nzi9Eosj5hUgUOb8QiSLnFyJR5PxCJIqcX4hE+f/LLrTxsD8+mgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4868fcb050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48695f5690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(graph = computation_graph) as sess:\n",
    "    # load the weights from the model1\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # instead of global variable initializer, restore the graph:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    prediction = sess.graph.get_tensor_by_name(\"prediction:0\")\n",
    "    inputs = sess.graph.get_tensor_by_name(\"inputs:0\")\n",
    "    \n",
    "    random_image = batch_data[np.random.randint(len(batch_data))]\n",
    "    reconstructed_image = sess.run(prediction, feed_dict={inputs: np.array([random_image])})[0]\n",
    "    \n",
    "    print random_image[:12, :12, 0]\n",
    "    print reconstructed_image[:12, :12, 0]\n",
    "    \n",
    "    # plot the two images with their titles:\n",
    "    plt.figure().suptitle(\"Original Image\")\n",
    "    plt.imshow(random_image, interpolation='none')\n",
    "    \n",
    "    print reconstructed_image.shape\n",
    "    \n",
    "    plt.figure().suptitle(\"Reconstructed Image\")\n",
    "    plt.imshow(reconstructed_image, interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Use the 2nd model that has pooling layers to check the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2, 2, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(32), Dimension(32), Dimension(3)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the graph from the Graph1 module \n",
    "import computation_graph.Graph2\n",
    "\n",
    "computation_graph = computation_graph.Graph2.graph\n",
    "\n",
    "# obtain a handle on the encoded_representation tensor of the dataflow computation graph\n",
    "encoded_representation = computation_graph.get_tensor_by_name(\"encoded_representation:0\")\n",
    "print encoded_representation.shape # The output shape of the encoded representation. It is 32 x 2 x 2 i.e 128 \n",
    "# Thus the latent representation is 128 dimensional\n",
    "\n",
    "# The prediction is again a good shaped tensor of dimensions 32 x 32 x 3\n",
    "prediction = computation_graph.get_tensor_by_name(\"prediction:0\")\n",
    "prediction.shape # This is same as the original image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_model_path = os.path.join(base_model_path, \"Model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell. Since, the data used for this task is CIFAR-10, \n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "    \n",
    "    It took me around 5hrs to execute this cell entirely.\n",
    "'''\n",
    "\n",
    "with tf.Session(graph=computation_graph) as sess:\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "         # load the weights from the model2\n",
    "        saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    else:\n",
    "        # create a new saver\n",
    "        saver = tf.train.Saver(max_to_keep=2)\n",
    "        \n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for ep in range(50, no_of_epochs):  # epochs loop\n",
    "        \n",
    "        print \"epoch: \" + str(ep + 1)\n",
    "        print \"=================================================================================================\"\n",
    "        print \"=================================================================================================\"\n",
    "        \n",
    "        for batch_n in range(no_of_batches):  # batches loop\n",
    "            \n",
    "            # retrieve the operations from the graph to be evaluated\n",
    "            loss = sess.graph.get_tensor_by_name(\"loss:0\")\n",
    "            train_op = sess.graph.get_operation_by_name(\"train_op\")\n",
    "            inputs = sess.graph.get_tensor_by_name(\"inputs:0\")\n",
    "            \n",
    "            # generate the batch images and labels\n",
    "            batch_images, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_\" + str(batch_n + 1)))\n",
    "            \n",
    "            min_batch_size = 2000 # we look at only 500 images at a time since the machine is small\n",
    "            \n",
    "            print \"current_batch: \" + str(batch_n + 1)\n",
    "            \n",
    "            for index in range(len(batch_images) / min_batch_size):\n",
    "                start = index * min_batch_size\n",
    "                end = start + min_batch_size\n",
    "                _, cost = sess.run([train_op, loss], feed_dict={inputs: batch_images[start: end]})\n",
    "                print('range:{} loss= {}'.format((start, end), cost))\n",
    "            \n",
    "            print \"\\n=========================================================================================\\n\"\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0):\n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(model_path, \"model1\"), global_step = (ep + 1))\n",
    "        \n",
    "    print \"=================================================================================================\"\n",
    "    print \"=================================================================================================\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
