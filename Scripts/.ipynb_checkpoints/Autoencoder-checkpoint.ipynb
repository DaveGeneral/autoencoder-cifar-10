{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to train an AutoEncoder for the CIFAR-10 dataset. \n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "### Technology used: Tensorflow-core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for machine learning\n",
    "import tensorflow as tf\n",
    "\n",
    "# packages used for processing: \n",
    "from six.moves import cPickle as pickle # for reading the data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder # for encoding the labels in one hot form\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Models\n",
      "README.md\n",
      "requirements.txt\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data/cifar-10-batches-py\" # the data path\n",
    "train_meta = os.path.join(data_path, \"batches.meta\")\n",
    "base_model_path = '../Models'\n",
    "\n",
    "# constant values:\n",
    "size = 32 # the images of size 32 x 32\n",
    "channels = 3 # RGB channels\n",
    "highest_pixel_value = 255.0 # 8 bits for every channel. So, max value is 255\n",
    "no_of_epochs = 100 # No. of epochs to run\n",
    "no_of_batches = 5 # There are 5 batches in the dataset\n",
    "checkpoint_factor = 5 # save the model after every 5 steps (epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta\n",
      "data_batch_1\n",
      "data_batch_2\n",
      "data_batch_3\n",
      "data_batch_4\n",
      "data_batch_5\n",
      "readme.html\n",
      "test_batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the contents inside the data folder\n",
    "exec_command(['ls', data_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unPickle a file: \n",
    "def unpickle(file):\n",
    "    '''\n",
    "        This function takes the file path and unPickles the file acquired from it\n",
    "        @Param file: the string path of the file\n",
    "        @return: The dict object unPickled from the file\n",
    "    '''\n",
    "    import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the contents of the batches.meta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_names': ['airplane',\n",
       "  'automobile',\n",
       "  'bird',\n",
       "  'cat',\n",
       "  'deer',\n",
       "  'dog',\n",
       "  'frog',\n",
       "  'horse',\n",
       "  'ship',\n",
       "  'truck'],\n",
       " 'num_cases_per_batch': 10000,\n",
       " 'num_vis': 3072}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = unpickle(train_meta)\n",
    "\n",
    "# check it's contents\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read and display some of the images from the dataset along with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'labels', 'batch_label', 'filenames']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_preliminary = unpickle(os.path.join(data_path, \"data_batch_3\"))\n",
    "\n",
    "# check it's contents\n",
    "train_batch_preliminary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[178, 191, 193, 197, 202, 206, 207, 209, 214, 219],\n",
       "       [140, 151, 155, 160, 166, 172, 173, 171, 176, 180],\n",
       "       [ 84,  94, 119, 151, 146, 127, 125, 135, 139, 139],\n",
       "       [ 16,  18,  85, 200, 207, 133,  71,  59,  72,  79],\n",
       "       [  9,   3,  51, 183, 238, 219, 177,  94,  30,  16],\n",
       "       [ 31,  25,  38, 148, 240, 249, 255, 235, 139,  39],\n",
       "       [ 69,  65,  62, 115, 215, 250, 248, 253, 245, 201],\n",
       "       [ 92,  89,  81,  89, 173, 240, 249, 253, 253, 255],\n",
       "       [ 93,  90,  84,  85, 139, 217, 241, 246, 251, 252],\n",
       "       [ 75,  74,  71,  87, 154, 208, 229, 239, 245, 250]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first 3 images from the dataset\n",
    "preliminary_data = train_batch_preliminary['data'].reshape((len(train_batch_preliminary['data']), 32, 32, 3), \n",
    "                                                           order='F')\n",
    "preliminary_labels = train_batch_preliminary['labels']\n",
    "\n",
    "# view some of the data:\n",
    "preliminary_data[33, :10, :10, 2] #(10 x 10) data of blue channel of 33rd image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 0, 6, 9, 2, 8, 3, 6, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a few values of the labels of the dataset\n",
    "preliminary_labels[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntJT/D8vcWt5jTyghAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f462f031950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEVCAYAAAAPaTtOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n42QYdwCOk2HcAThOhvn/mwRmPNmOpJYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f467896e1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8v8BILUB46oFV8MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4632af2e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "    plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "    plt.imshow(preliminary_data[random_index], interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images look blurred out because they are very low resolution images (32 x 32) pixels only.\n",
    "\n",
    "## It can be seen that the images in the original dataset are skewed. So, we will have to rotate them by 90 degrees clockwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njpNS3PkdJ6X8fxYsM+VTbYvIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46328df9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try using the numpy.rot90 method for this:\n",
    "random_index = np.random.randint(preliminary_data.shape[0])\n",
    "    \n",
    "plt.figure().suptitle(\"Random Image from the dataset: %s\" %(meta_data['label_names'][preliminary_labels[random_index]]))\n",
    "plt.imshow(np.rot90(preliminary_data[random_index], axes=(1, 0)), interpolation='none'); # suppress the unnecessary\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works. So, now we can create a function to put all this together. This function would take the batch pickle file and create the data suitable for feeding it off to a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The batch generator function:\n",
    "def generateBatch(batchFile):\n",
    "    '''\n",
    "        The function to generate a batch of data suitable for performing the convNet operations on it\n",
    "        @param batchFile -> the path of the input batchfile\n",
    "        @return batch: (data, labels) -> the processed data.\n",
    "    '''\n",
    "    # unpickle the batch file:\n",
    "    data_dict = unpickle(batchFile)\n",
    "    \n",
    "    # extract the data and labels from this dictionary\n",
    "    unprocessed_data = data_dict['data']\n",
    "    integer_labels = np.array(data_dict['labels']) # labels in integer form\n",
    "    \n",
    "    # reshape and rotate the data\n",
    "    data = unprocessed_data.reshape((len(unprocessed_data), size, size, channels), order='F')\n",
    "    processed_data = np.array(map(lambda x: np.rot90(x, axes=(1, 0)), data))\n",
    "    \n",
    "    # normalize the images by dividing all the pixels by 255\n",
    "    processed_data = processed_data / 255\n",
    "    \n",
    "    # encode the labels in one-hot encoded form\n",
    "    # we use the sklearn.preprocessing package for doing this\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoded_labels = np.array(encoder.fit_transform(integer_labels.reshape(len(integer_labels), 1))).astype(np.int)\n",
    "    \n",
    "    # return the processed data and the encoded_labels:\n",
    "    return (processed_data, encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "# load the batch no. 1 and check if it works correctly.\n",
    "batch_data, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_1\"))\n",
    "print (batch_data.shape, batch_labels.shape)\n",
    "\n",
    "# batch_data[0, :12, :12, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! So, the data extraction module is setup. Let's move on to the actual model building and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The computation graph (tensorflow graph and not the weights) for this model is inside the computation_graph package. It can be imported from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(2), Dimension(2), Dimension(32)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the graph from the Graph1 module \n",
    "import computation_graph.Graph1 \n",
    "\n",
    "computation_graph = computation_graph.Graph1.graph\n",
    "\n",
    "# obtain a handle on the encoded_representation tensor of the dataflow computation graph\n",
    "encoded_representation = computation_graph.get_tensor_by_name(\"encoded_representation:0\")\n",
    "encoded_representation.shape # The output shape of the encoded representation. It is 32 x 2 x 2 i.e 128 \n",
    "# Thus the latent representation is 128 dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(base_model_path, \"Model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's write the session code to run this computation graph and perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell. Since, the data used for this task is CIFAR-10, \n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "    \n",
    "    It took me around 5hrs to execute this cell entirely.\n",
    "'''\n",
    "\n",
    "with tf.Session(graph=computation_graph) as sess:\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "         # load the weights from the model1\n",
    "        saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    else:\n",
    "        # create a new saver\n",
    "        saver = tf.train.Saver(max_to_keep=2)\n",
    "        \n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for ep in range(50, no_of_epochs):  # epochs loop\n",
    "        \n",
    "        print \"epoch: \" + str(ep + 1)\n",
    "        print \"=================================================================================================\"\n",
    "        print \"=================================================================================================\"\n",
    "        \n",
    "        for batch_n in range(no_of_batches):  # batches loop\n",
    "            \n",
    "            # retrieve the operations from the graph to be evaluated\n",
    "            loss = sess.graph.get_tensor_by_name(\"loss:0\")\n",
    "            train_op = sess.graph.get_operation_by_name(\"train_op\")\n",
    "            inputs = sess.graph.get_tensor_by_name(\"inputs:0\")\n",
    "            \n",
    "            # generate the batch images and labels\n",
    "            batch_images, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_\" + str(batch_n + 1)))\n",
    "            \n",
    "            min_batch_size = 2000 # we look at only 500 images at a time since the machine is small\n",
    "            \n",
    "            print \"current_batch: \" + str(batch_n + 1)\n",
    "            \n",
    "            for index in range(len(batch_images) / min_batch_size):\n",
    "                start = index * min_batch_size\n",
    "                end = start + min_batch_size\n",
    "                _, cost = sess.run([train_op, loss], feed_dict={inputs: batch_images[start: end]})\n",
    "                print('range:{} loss= {}'.format((start, end), cost))\n",
    "            \n",
    "            print \"\\n=========================================================================================\\n\"\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0):\n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(model_path, \"model1\"), global_step = (ep + 1))\n",
    "        \n",
    "    print \"=================================================================================================\"\n",
    "    print \"=================================================================================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's visualize the representation of a random image and it's reconstructed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Models/Model1/model1-100\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niET5f7yVc7SyzCqEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f55381ec450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nG4ZP+X9+RKOaMDs9eQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f552f335450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(graph = computation_graph) as sess:\n",
    "    # load the weights from the model1\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # instead of global variable initializer, restore the graph:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    prediction = sess.graph.get_tensor_by_name(\"prediction:0\")\n",
    "    inputs = sess.graph.get_tensor_by_name(\"inputs:0\")\n",
    "    \n",
    "    random_image = batch_data[np.random.randint(len(batch_data))]\n",
    "    reconstructed_image = sess.run(prediction, feed_dict={inputs: np.array([random_image])})[0]\n",
    "    \n",
    "    # plot the two images with their titles:\n",
    "    plt.figure().suptitle(\"Original Image\")\n",
    "    plt.imshow(random_image, interpolation='none')\n",
    "    \n",
    "    plt.figure().suptitle(\"Reconstructed Image\")\n",
    "    plt.imshow(reconstructed_image, interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Use the 2nd model that has pooling layers to check the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(2), Dimension(2), Dimension(32)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the graph from the Graph1 module \n",
    "import computation_graph.Graph2\n",
    "\n",
    "computation_graph = computation_graph.Graph2.graph\n",
    "\n",
    "# obtain a handle on the encoded_representation tensor of the dataflow computation graph\n",
    "encoded_representation = computation_graph.get_tensor_by_name(\"encoded_representation:0\")\n",
    "encoded_representation.shape # The output shape of the encoded representation. It is 32 x 2 x 2 i.e 128 \n",
    "# Thus the latent representation is 128 dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_model_path = os.path.join(base_model_path, \"Model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Models/Model2/model2-300\n",
      "epoch: 301\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2643985748\n",
      "range:(5000, 10000) loss= 14.2357521057\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2732048035\n",
      "range:(5000, 10000) loss= 14.3922719955\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2934865952\n",
      "range:(5000, 10000) loss= 14.3799219131\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.2250728607\n",
      "range:(5000, 10000) loss= 14.2615604401\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1892538071\n",
      "range:(5000, 10000) loss= 14.1597518921\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 302\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2748699188\n",
      "range:(5000, 10000) loss= 14.2478103638\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2822961807\n",
      "range:(5000, 10000) loss= 14.3957118988\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2885169983\n",
      "range:(5000, 10000) loss= 14.3680200577\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.2109966278\n",
      "range:(5000, 10000) loss= 14.2467002869\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1758289337\n",
      "range:(5000, 10000) loss= 14.1462554932\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 303\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2607574463\n",
      "range:(5000, 10000) loss= 14.2308454514\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.265832901\n",
      "range:(5000, 10000) loss= 14.3836164474\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2855262756\n",
      "range:(5000, 10000) loss= 14.3715085983\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.2167215347\n",
      "range:(5000, 10000) loss= 14.2534761429\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1827802658\n",
      "range:(5000, 10000) loss= 14.1546897888\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 304\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2686767578\n",
      "range:(5000, 10000) loss= 14.2386131287\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2723770142\n",
      "range:(5000, 10000) loss= 14.3881225586\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.282869339\n",
      "range:(5000, 10000) loss= 14.3617238998\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.2049455643\n",
      "range:(5000, 10000) loss= 14.2413244247\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1711454391\n",
      "range:(5000, 10000) loss= 14.141078949\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 305\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2531290054\n",
      "range:(5000, 10000) loss= 14.2202978134\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2535448074\n",
      "range:(5000, 10000) loss= 14.3703393936\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2731714249\n",
      "range:(5000, 10000) loss= 14.3614063263\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.2112045288\n",
      "range:(5000, 10000) loss= 14.251625061\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1822795868\n",
      "range:(5000, 10000) loss= 14.1538066864\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 306\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2639741898\n",
      "range:(5000, 10000) loss= 14.2300081253\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2614946365\n",
      "range:(5000, 10000) loss= 14.3773117065\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2716398239\n",
      "range:(5000, 10000) loss= 14.349064827\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1947660446\n",
      "range:(5000, 10000) loss= 14.2353143692\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1692037582\n",
      "range:(5000, 10000) loss= 14.1404266357\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 307\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2525272369\n",
      "range:(5000, 10000) loss= 14.2177724838\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2425937653\n",
      "range:(5000, 10000) loss= 14.3423776627\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2344226837\n",
      "range:(5000, 10000) loss= 14.3193559647\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1836957932\n",
      "range:(5000, 10000) loss= 14.2410125732\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1875486374\n",
      "range:(5000, 10000) loss= 14.1700601578\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 308\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2780427933\n",
      "range:(5000, 10000) loss= 14.2299108505\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2397956848\n",
      "range:(5000, 10000) loss= 14.3372745514\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2324457169\n",
      "range:(5000, 10000) loss= 14.3173761368\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1785440445\n",
      "range:(5000, 10000) loss= 14.2374296188\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1888647079\n",
      "range:(5000, 10000) loss= 14.1685628891\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 309\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2752962112\n",
      "range:(5000, 10000) loss= 14.2232570648\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2240543365\n",
      "range:(5000, 10000) loss= 14.297170639\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1764974594\n",
      "range:(5000, 10000) loss= 14.2511844635\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1196374893\n",
      "range:(5000, 10000) loss= 14.1808376312\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1412057877\n",
      "range:(5000, 10000) loss= 14.1447925568\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 310\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2888288498\n",
      "range:(5000, 10000) loss= 14.2744436264\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.3025465012\n",
      "range:(5000, 10000) loss= 14.4126329422\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2916622162\n",
      "range:(5000, 10000) loss= 14.3596029282\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1865940094\n",
      "range:(5000, 10000) loss= 14.2089014053\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1290855408\n",
      "range:(5000, 10000) loss= 14.0855350494\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 311\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1918983459\n",
      "range:(5000, 10000) loss= 14.1549701691\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1837654114\n",
      "range:(5000, 10000) loss= 14.2974081039\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2182970047\n",
      "range:(5000, 10000) loss= 14.3314809799\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.2164239883\n",
      "range:(5000, 10000) loss= 14.2914123535\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.2399864197\n",
      "range:(5000, 10000) loss= 14.2184104919\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 312\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.3143434525\n",
      "range:(5000, 10000) loss= 14.2714252472\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2781476974\n",
      "range:(5000, 10000) loss= 14.3586044312\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2270822525\n",
      "range:(5000, 10000) loss= 14.2964105606\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1507320404\n",
      "range:(5000, 10000) loss= 14.1980600357\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1386260986\n",
      "range:(5000, 10000) loss= 14.1147794724\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 313\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2356348038\n",
      "range:(5000, 10000) loss= 14.2098379135\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2454376221\n",
      "range:(5000, 10000) loss= 14.3616600037\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2559833527\n",
      "range:(5000, 10000) loss= 14.3383626938\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1883916855\n",
      "range:(5000, 10000) loss= 14.2310829163\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1583766937\n",
      "range:(5000, 10000) loss= 14.114654541\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 314\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2150144577\n",
      "range:(5000, 10000) loss= 14.1711702347\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1886539459\n",
      "range:(5000, 10000) loss= 14.2797060013\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1748685837\n",
      "range:(5000, 10000) loss= 14.2642669678\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1418085098\n",
      "range:(5000, 10000) loss= 14.2157649994\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1857481003\n",
      "range:(5000, 10000) loss= 14.1926574707\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 315\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.314666748\n",
      "range:(5000, 10000) loss= 14.2817020416\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2954864502\n",
      "range:(5000, 10000) loss= 14.3842420578\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2442951202\n",
      "range:(5000, 10000) loss= 14.2977256775\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1343173981\n",
      "range:(5000, 10000) loss= 14.1703920364\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1072273254\n",
      "range:(5000, 10000) loss= 14.0808496475\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 316\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2003231049\n",
      "range:(5000, 10000) loss= 14.1688156128\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1958703995\n",
      "range:(5000, 10000) loss= 14.2958126068\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.189037323\n",
      "range:(5000, 10000) loss= 14.2704648972\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1373996735\n",
      "range:(5000, 10000) loss= 14.2081642151\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1823663712\n",
      "range:(5000, 10000) loss= 14.179236412\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 317\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2837266922\n",
      "range:(5000, 10000) loss= 14.2244739532\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2293519974\n",
      "range:(5000, 10000) loss= 14.3401584625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2451896667\n",
      "range:(5000, 10000) loss= 14.326002121\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1625156403\n",
      "range:(5000, 10000) loss= 14.1932601929\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1284246445\n",
      "range:(5000, 10000) loss= 14.1114768982\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 318\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2394113541\n",
      "range:(5000, 10000) loss= 14.202082634\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.210187912\n",
      "range:(5000, 10000) loss= 14.2990121841\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2047548294\n",
      "range:(5000, 10000) loss= 14.3105688095\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.176074028\n",
      "range:(5000, 10000) loss= 14.2218294144\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1486625671\n",
      "range:(5000, 10000) loss= 14.1140832901\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 319\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2395935059\n",
      "range:(5000, 10000) loss= 14.2341146469\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2735376358\n",
      "range:(5000, 10000) loss= 14.377702713\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2491149902\n",
      "range:(5000, 10000) loss= 14.3237495422\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.160826683\n",
      "range:(5000, 10000) loss= 14.1779851913\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0952892303\n",
      "range:(5000, 10000) loss= 14.0576705933\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 320\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1789579391\n",
      "range:(5000, 10000) loss= 14.160279274\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1960391998\n",
      "range:(5000, 10000) loss= 14.2972393036\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1969261169\n",
      "range:(5000, 10000) loss= 14.301232338\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1806898117\n",
      "range:(5000, 10000) loss= 14.2361679077\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1660394669\n",
      "range:(5000, 10000) loss= 14.1417474747\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 321\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2766208649\n",
      "range:(5000, 10000) loss= 14.2603120804\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2579059601\n",
      "range:(5000, 10000) loss= 14.3340072632\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2130298615\n",
      "range:(5000, 10000) loss= 14.2894086838\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1301183701\n",
      "range:(5000, 10000) loss= 14.1575832367\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0920495987\n",
      "range:(5000, 10000) loss= 14.0736045837\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 322\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2078285217\n",
      "range:(5000, 10000) loss= 14.1856660843\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2091856003\n",
      "range:(5000, 10000) loss= 14.3183116913\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2323856354\n",
      "range:(5000, 10000) loss= 14.3357315063\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1791391373\n",
      "range:(5000, 10000) loss= 14.2070016861\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.13306427\n",
      "range:(5000, 10000) loss= 14.1056165695\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 323\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2294979095\n",
      "range:(5000, 10000) loss= 14.195192337\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2072162628\n",
      "range:(5000, 10000) loss= 14.3085889816\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2149124146\n",
      "range:(5000, 10000) loss= 14.3138637543\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1587610245\n",
      "range:(5000, 10000) loss= 14.1861782074\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1127500534\n",
      "range:(5000, 10000) loss= 14.0868816376\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 324\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2203121185\n",
      "range:(5000, 10000) loss= 14.1917514801\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2031927109\n",
      "range:(5000, 10000) loss= 14.3058977127\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2170467377\n",
      "range:(5000, 10000) loss= 14.3184700012\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1583309174\n",
      "range:(5000, 10000) loss= 14.1786355972\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1040620804\n",
      "range:(5000, 10000) loss= 14.0799627304\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 325\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2127285004\n",
      "range:(5000, 10000) loss= 14.1720905304\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1785707474\n",
      "range:(5000, 10000) loss= 14.281083107\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1993017197\n",
      "range:(5000, 10000) loss= 14.3059749603\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1544790268\n",
      "range:(5000, 10000) loss= 14.185008049\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1174058914\n",
      "range:(5000, 10000) loss= 14.1012229919\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 326\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2269239426\n",
      "range:(5000, 10000) loss= 14.1772899628\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1870393753\n",
      "range:(5000, 10000) loss= 14.2952041626\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2047348022\n",
      "range:(5000, 10000) loss= 14.2956027985\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1364946365\n",
      "range:(5000, 10000) loss= 14.1698217392\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1133441925\n",
      "range:(5000, 10000) loss= 14.0953521729\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 327\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2022981644\n",
      "range:(5000, 10000) loss= 14.1424808502\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1579122543\n",
      "range:(5000, 10000) loss= 14.2672052383\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1829061508\n",
      "range:(5000, 10000) loss= 14.2816724777\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.142241478\n",
      "range:(5000, 10000) loss= 14.1991357803\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1464414597\n",
      "range:(5000, 10000) loss= 14.118227005\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 328\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2273397446\n",
      "range:(5000, 10000) loss= 14.1908273697\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2188482285\n",
      "range:(5000, 10000) loss= 14.3295402527\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2144775391\n",
      "range:(5000, 10000) loss= 14.2888689041\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1374197006\n",
      "range:(5000, 10000) loss= 14.1716270447\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1028642654\n",
      "range:(5000, 10000) loss= 14.0657186508\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 329\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1673908234\n",
      "range:(5000, 10000) loss= 14.1159830093\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1313142776\n",
      "range:(5000, 10000) loss= 14.2205600739\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1264352798\n",
      "range:(5000, 10000) loss= 14.2130708694\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0738105774\n",
      "range:(5000, 10000) loss= 14.1162843704\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0516252518\n",
      "range:(5000, 10000) loss= 14.0147819519\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 330\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1221323013\n",
      "range:(5000, 10000) loss= 14.0858373642\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1179523468\n",
      "range:(5000, 10000) loss= 14.2200126648\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.123251915\n",
      "range:(5000, 10000) loss= 14.202290535\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0689172745\n",
      "range:(5000, 10000) loss= 14.1185560226\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0698413849\n",
      "range:(5000, 10000) loss= 14.0694894791\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 331\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2438631058\n",
      "range:(5000, 10000) loss= 14.2412014008\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2979049683\n",
      "range:(5000, 10000) loss= 14.393579483\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.3091440201\n",
      "range:(5000, 10000) loss= 14.3305997849\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1392917633\n",
      "range:(5000, 10000) loss= 14.1468830109\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0844764709\n",
      "range:(5000, 10000) loss= 14.0659313202\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 332\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1865644455\n",
      "range:(5000, 10000) loss= 14.1353769302\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.150557518\n",
      "range:(5000, 10000) loss= 14.2609615326\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2144365311\n",
      "range:(5000, 10000) loss= 14.3302335739\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.2182044983\n",
      "range:(5000, 10000) loss= 14.2590913773\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.2184228897\n",
      "range:(5000, 10000) loss= 14.1613512039\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 333\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2526397705\n",
      "range:(5000, 10000) loss= 14.1815910339\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.2033662796\n",
      "range:(5000, 10000) loss= 14.2942171097\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1998586655\n",
      "range:(5000, 10000) loss= 14.2737979889\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1432981491\n",
      "range:(5000, 10000) loss= 14.1881456375\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1270809174\n",
      "range:(5000, 10000) loss= 14.088891983\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 334\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.21397686\n",
      "range:(5000, 10000) loss= 14.1836080551\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.23554039\n",
      "range:(5000, 10000) loss= 14.3350534439\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2477827072\n",
      "range:(5000, 10000) loss= 14.2965354919\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1249475479\n",
      "range:(5000, 10000) loss= 14.1434144974\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0747117996\n",
      "range:(5000, 10000) loss= 14.037607193\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 335\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1492700577\n",
      "range:(5000, 10000) loss= 14.1069831848\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1416482925\n",
      "range:(5000, 10000) loss= 14.2588434219\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2011852264\n",
      "range:(5000, 10000) loss= 14.2996177673\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1707849503\n",
      "range:(5000, 10000) loss= 14.2027788162\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1369686127\n",
      "range:(5000, 10000) loss= 14.0899200439\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 336\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2043457031\n",
      "range:(5000, 10000) loss= 14.1588935852\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1902503967\n",
      "range:(5000, 10000) loss= 14.28221035\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1947183609\n",
      "range:(5000, 10000) loss= 14.2693767548\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1218662262\n",
      "range:(5000, 10000) loss= 14.1510896683\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0877885818\n",
      "range:(5000, 10000) loss= 14.0559415817\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 337\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1802129745\n",
      "range:(5000, 10000) loss= 14.1474876404\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1921272278\n",
      "range:(5000, 10000) loss= 14.2996168137\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2231302261\n",
      "range:(5000, 10000) loss= 14.2865962982\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1239461899\n",
      "range:(5000, 10000) loss= 14.1419687271\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0727748871\n",
      "range:(5000, 10000) loss= 14.038983345\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 338\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1585416794\n",
      "range:(5000, 10000) loss= 14.1209154129\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.156378746\n",
      "range:(5000, 10000) loss= 14.2651748657\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1983451843\n",
      "range:(5000, 10000) loss= 14.2828931808\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1384363174\n",
      "range:(5000, 10000) loss= 14.1629085541\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0990772247\n",
      "range:(5000, 10000) loss= 14.0642147064\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 339\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1819725037\n",
      "range:(5000, 10000) loss= 14.1350975037\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1624355316\n",
      "range:(5000, 10000) loss= 14.2616271973\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1830215454\n",
      "range:(5000, 10000) loss= 14.2614574432\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1171503067\n",
      "range:(5000, 10000) loss= 14.1479187012\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0878915787\n",
      "range:(5000, 10000) loss= 14.0566062927\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 340\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1732263565\n",
      "range:(5000, 10000) loss= 14.1273803711\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1574249268\n",
      "range:(5000, 10000) loss= 14.2615814209\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1866903305\n",
      "range:(5000, 10000) loss= 14.2633733749\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1166601181\n",
      "range:(5000, 10000) loss= 14.1461791992\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0854539871\n",
      "range:(5000, 10000) loss= 14.0522527695\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 341\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1653499603\n",
      "range:(5000, 10000) loss= 14.1177940369\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1468086243\n",
      "range:(5000, 10000) loss= 14.2503709793\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1751537323\n",
      "range:(5000, 10000) loss= 14.2569055557\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1148662567\n",
      "range:(5000, 10000) loss= 14.1458730698\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0871038437\n",
      "range:(5000, 10000) loss= 14.0547304153\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 342\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1704473495\n",
      "range:(5000, 10000) loss= 14.1196260452\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.142537117\n",
      "range:(5000, 10000) loss= 14.2417430878\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.163441658\n",
      "range:(5000, 10000) loss= 14.2435445786\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1018705368\n",
      "range:(5000, 10000) loss= 14.1368732452\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0829105377\n",
      "range:(5000, 10000) loss= 14.052778244\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 343\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1642475128\n",
      "range:(5000, 10000) loss= 14.1110210419\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1340065002\n",
      "range:(5000, 10000) loss= 14.2376060486\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1625738144\n",
      "range:(5000, 10000) loss= 14.2421627045\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1015901566\n",
      "range:(5000, 10000) loss= 14.1387853622\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0862636566\n",
      "range:(5000, 10000) loss= 14.0524358749\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 344\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1570224762\n",
      "range:(5000, 10000) loss= 14.1025667191\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1278934479\n",
      "range:(5000, 10000) loss= 14.2357110977\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1637153625\n",
      "range:(5000, 10000) loss= 14.2432498932\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1015195847\n",
      "range:(5000, 10000) loss= 14.1384983063\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0844478607\n",
      "range:(5000, 10000) loss= 14.0480632782\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 345\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1494626999\n",
      "range:(5000, 10000) loss= 14.0942487717\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.121465683\n",
      "range:(5000, 10000) loss= 14.2322349548\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1622591019\n",
      "range:(5000, 10000) loss= 14.2414855957\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1000938416\n",
      "range:(5000, 10000) loss= 14.1389331818\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0841913223\n",
      "range:(5000, 10000) loss= 14.0439023972\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 346\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1422624588\n",
      "range:(5000, 10000) loss= 14.087808609\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1176748276\n",
      "range:(5000, 10000) loss= 14.2296228409\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1571836472\n",
      "range:(5000, 10000) loss= 14.2335691452\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0918674469\n",
      "range:(5000, 10000) loss= 14.1330270767\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0798816681\n",
      "range:(5000, 10000) loss= 14.0389232635\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 347\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1367855072\n",
      "range:(5000, 10000) loss= 14.0827732086\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1120853424\n",
      "range:(5000, 10000) loss= 14.2213277817\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1471500397\n",
      "range:(5000, 10000) loss= 14.2292308807\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0983257294\n",
      "range:(5000, 10000) loss= 14.1445550919\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0887775421\n",
      "range:(5000, 10000) loss= 14.041176796\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 348\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1391906738\n",
      "range:(5000, 10000) loss= 14.0905313492\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1245393753\n",
      "range:(5000, 10000) loss= 14.2283086777\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1439962387\n",
      "range:(5000, 10000) loss= 14.2162208557\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0793066025\n",
      "range:(5000, 10000) loss= 14.1252698898\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0682878494\n",
      "range:(5000, 10000) loss= 14.0236978531\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 349\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1263046265\n",
      "range:(5000, 10000) loss= 14.084810257\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1254081726\n",
      "range:(5000, 10000) loss= 14.234462738\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.148806572\n",
      "range:(5000, 10000) loss= 14.2179498672\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.07234478\n",
      "range:(5000, 10000) loss= 14.1045770645\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0400295258\n",
      "range:(5000, 10000) loss= 14.005657196\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 350\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1244850159\n",
      "range:(5000, 10000) loss= 14.095916748\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1339645386\n",
      "range:(5000, 10000) loss= 14.2187461853\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1000709534\n",
      "range:(5000, 10000) loss= 14.1492061615\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.002904892\n",
      "range:(5000, 10000) loss= 14.0492210388\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9937667847\n",
      "range:(5000, 10000) loss= 13.9638204575\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 351\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0732603073\n",
      "range:(5000, 10000) loss= 14.0311899185\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0595560074\n",
      "range:(5000, 10000) loss= 14.168923378\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.091794014\n",
      "range:(5000, 10000) loss= 14.1675271988\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0134057999\n",
      "range:(5000, 10000) loss= 14.0507888794\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.00431633\n",
      "range:(5000, 10000) loss= 14.0102357864\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 352\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1709003448\n",
      "range:(5000, 10000) loss= 14.1719684601\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1985464096\n",
      "range:(5000, 10000) loss= 14.2639045715\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1322174072\n",
      "range:(5000, 10000) loss= 14.2396678925\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1161222458\n",
      "range:(5000, 10000) loss= 14.1401853561\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0505933762\n",
      "range:(5000, 10000) loss= 14.0116691589\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 353\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1371593475\n",
      "range:(5000, 10000) loss= 14.1174936295\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1543989182\n",
      "range:(5000, 10000) loss= 14.2773122787\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.2061271667\n",
      "range:(5000, 10000) loss= 14.3002023697\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1199789047\n",
      "range:(5000, 10000) loss= 14.138885498\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0702037811\n",
      "range:(5000, 10000) loss= 14.0378017426\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 354\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1388587952\n",
      "range:(5000, 10000) loss= 14.09146595\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1307563782\n",
      "range:(5000, 10000) loss= 14.2667961121\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1644124985\n",
      "range:(5000, 10000) loss= 14.2500133514\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1079025269\n",
      "range:(5000, 10000) loss= 14.1440696716\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.059425354\n",
      "range:(5000, 10000) loss= 14.0068330765\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 355\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1103086472\n",
      "range:(5000, 10000) loss= 14.0654449463\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0788698196\n",
      "range:(5000, 10000) loss= 14.1658353806\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0892972946\n",
      "range:(5000, 10000) loss= 14.218462944\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0977487564\n",
      "range:(5000, 10000) loss= 14.1433229446\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1014842987\n",
      "range:(5000, 10000) loss= 14.114522934\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 356\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.223742485\n",
      "range:(5000, 10000) loss= 14.1706914902\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.198132515\n",
      "range:(5000, 10000) loss= 14.2899188995\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.128610611\n",
      "range:(5000, 10000) loss= 14.1821622849\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0505313873\n",
      "range:(5000, 10000) loss= 14.090886116\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0130271912\n",
      "range:(5000, 10000) loss= 13.9770679474\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 357\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1015501022\n",
      "range:(5000, 10000) loss= 14.0684709549\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0965070724\n",
      "range:(5000, 10000) loss= 14.2031059265\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1238870621\n",
      "range:(5000, 10000) loss= 14.2421922684\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1026582718\n",
      "range:(5000, 10000) loss= 14.14199543\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0880899429\n",
      "range:(5000, 10000) loss= 14.0682458878\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 358\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1679191589\n",
      "range:(5000, 10000) loss= 14.0967540741\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1050815582\n",
      "range:(5000, 10000) loss= 14.2016906738\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0886564255\n",
      "range:(5000, 10000) loss= 14.1749267578\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0482063293\n",
      "range:(5000, 10000) loss= 14.087143898\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0071907043\n",
      "range:(5000, 10000) loss= 13.9738645554\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 359\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1123476028\n",
      "range:(5000, 10000) loss= 14.0884799957\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1045103073\n",
      "range:(5000, 10000) loss= 14.2025527954\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1383066177\n",
      "range:(5000, 10000) loss= 14.2629957199\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.1069250107\n",
      "range:(5000, 10000) loss= 14.1413974762\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0838499069\n",
      "range:(5000, 10000) loss= 14.0520582199\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 360\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1418809891\n",
      "range:(5000, 10000) loss= 14.0953292847\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.131737709\n",
      "range:(5000, 10000) loss= 14.2460746765\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1448068619\n",
      "range:(5000, 10000) loss= 14.232419014\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0710220337\n",
      "range:(5000, 10000) loss= 14.0982913971\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.029001236\n",
      "range:(5000, 10000) loss= 13.9980583191\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 361\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1135149002\n",
      "range:(5000, 10000) loss= 14.0781335831\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.121137619\n",
      "range:(5000, 10000) loss= 14.251414299\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.157330513\n",
      "range:(5000, 10000) loss= 14.2382154465\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0692167282\n",
      "range:(5000, 10000) loss= 14.0872917175\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0180597305\n",
      "range:(5000, 10000) loss= 13.99376297\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 362\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1018333435\n",
      "range:(5000, 10000) loss= 14.0335884094\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0307998657\n",
      "range:(5000, 10000) loss= 14.1108388901\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0249099731\n",
      "range:(5000, 10000) loss= 14.1332397461\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.029253006\n",
      "range:(5000, 10000) loss= 14.1156625748\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.1048288345\n",
      "range:(5000, 10000) loss= 14.1304759979\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 363\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.2479228973\n",
      "range:(5000, 10000) loss= 14.192527771\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.171541214\n",
      "range:(5000, 10000) loss= 14.2258272171\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.086514473\n",
      "range:(5000, 10000) loss= 14.1517553329\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0049295425\n",
      "range:(5000, 10000) loss= 14.0435562134\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9852895737\n",
      "range:(5000, 10000) loss= 13.9571371078\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 364\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.077504158\n",
      "range:(5000, 10000) loss= 14.0494165421\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.096414566\n",
      "range:(5000, 10000) loss= 14.2251777649\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1425790787\n",
      "range:(5000, 10000) loss= 14.2390184402\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.086435318\n",
      "range:(5000, 10000) loss= 14.1165437698\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.033416748\n",
      "range:(5000, 10000) loss= 13.9851551056\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 365\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0869436264\n",
      "range:(5000, 10000) loss= 14.0404939651\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.068561554\n",
      "range:(5000, 10000) loss= 14.1789360046\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0944099426\n",
      "range:(5000, 10000) loss= 14.1928186417\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0527143478\n",
      "range:(5000, 10000) loss= 14.0981407166\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0471019745\n",
      "range:(5000, 10000) loss= 14.0368833542\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 366\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1542024612\n",
      "range:(5000, 10000) loss= 14.1099100113\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1305522919\n",
      "range:(5000, 10000) loss= 14.2301139832\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.110499382\n",
      "range:(5000, 10000) loss= 14.174530983\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0181951523\n",
      "range:(5000, 10000) loss= 14.0574827194\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0024576187\n",
      "range:(5000, 10000) loss= 13.9799289703\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 367\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0994329453\n",
      "range:(5000, 10000) loss= 14.0526123047\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0652208328\n",
      "range:(5000, 10000) loss= 14.1619167328\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0841779709\n",
      "range:(5000, 10000) loss= 14.2014551163\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0681409836\n",
      "range:(5000, 10000) loss= 14.1066436768\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0395727158\n",
      "range:(5000, 10000) loss= 14.014799118\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 368\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.138053894\n",
      "range:(5000, 10000) loss= 14.0983791351\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1134662628\n",
      "range:(5000, 10000) loss= 14.2155437469\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1149950027\n",
      "range:(5000, 10000) loss= 14.1921787262\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0279512405\n",
      "range:(5000, 10000) loss= 14.0549459457\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9966535568\n",
      "range:(5000, 10000) loss= 13.9798135757\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 369\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0931453705\n",
      "range:(5000, 10000) loss= 14.0304117203\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.037856102\n",
      "range:(5000, 10000) loss= 14.1396789551\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0711317062\n",
      "range:(5000, 10000) loss= 14.1896266937\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0619449615\n",
      "range:(5000, 10000) loss= 14.1167087555\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0647268295\n",
      "range:(5000, 10000) loss= 14.0444822311\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 370\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.144944191\n",
      "range:(5000, 10000) loss= 14.0926818848\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1133079529\n",
      "range:(5000, 10000) loss= 14.2155590057\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1014499664\n",
      "range:(5000, 10000) loss= 14.1720533371\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0210752487\n",
      "range:(5000, 10000) loss= 14.0569391251\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9920787811\n",
      "range:(5000, 10000) loss= 13.9577608109\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 371\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0705022812\n",
      "range:(5000, 10000) loss= 14.0316648483\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.065533638\n",
      "range:(5000, 10000) loss= 14.1866350174\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1134357452\n",
      "range:(5000, 10000) loss= 14.2200832367\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0631685257\n",
      "range:(5000, 10000) loss= 14.0838479996\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0085849762\n",
      "range:(5000, 10000) loss= 13.976524353\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 372\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0955905914\n",
      "range:(5000, 10000) loss= 14.0653162003\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1029434204\n",
      "range:(5000, 10000) loss= 14.2204351425\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1187181473\n",
      "range:(5000, 10000) loss= 14.1913948059\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0236616135\n",
      "range:(5000, 10000) loss= 14.0473518372\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9800958633\n",
      "range:(5000, 10000) loss= 13.9508266449\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 373\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0697631836\n",
      "range:(5000, 10000) loss= 14.0379676819\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0773019791\n",
      "range:(5000, 10000) loss= 14.1986398697\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1127576828\n",
      "range:(5000, 10000) loss= 14.2022590637\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0416603088\n",
      "range:(5000, 10000) loss= 14.0657129288\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9939746857\n",
      "range:(5000, 10000) loss= 13.9616413116\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 374\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0820684433\n",
      "range:(5000, 10000) loss= 14.0530166626\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0877122879\n",
      "range:(5000, 10000) loss= 14.1958208084\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0973310471\n",
      "range:(5000, 10000) loss= 14.1829509735\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0287132263\n",
      "range:(5000, 10000) loss= 14.0527915955\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.979801178\n",
      "range:(5000, 10000) loss= 13.9469852448\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 375\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0725564957\n",
      "range:(5000, 10000) loss= 14.0483446121\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0781736374\n",
      "range:(5000, 10000) loss= 14.1778249741\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0810565948\n",
      "range:(5000, 10000) loss= 14.1744737625\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0286083221\n",
      "range:(5000, 10000) loss= 14.055431366\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9840745926\n",
      "range:(5000, 10000) loss= 13.9591827393\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 376\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0899896622\n",
      "range:(5000, 10000) loss= 14.0631809235\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0839567184\n",
      "range:(5000, 10000) loss= 14.1787309647\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0808582306\n",
      "range:(5000, 10000) loss= 14.1694393158\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.018040657\n",
      "range:(5000, 10000) loss= 14.0466690063\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9835290909\n",
      "range:(5000, 10000) loss= 13.9648113251\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 377\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.091504097\n",
      "range:(5000, 10000) loss= 14.0503425598\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0642061234\n",
      "range:(5000, 10000) loss= 14.1649551392\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0809831619\n",
      "range:(5000, 10000) loss= 14.1781339645\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0257406235\n",
      "range:(5000, 10000) loss= 14.052230835\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9895715714\n",
      "range:(5000, 10000) loss= 13.9707860947\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 378\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0937900543\n",
      "range:(5000, 10000) loss= 14.047372818\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.062376976\n",
      "range:(5000, 10000) loss= 14.1673707962\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0843038559\n",
      "range:(5000, 10000) loss= 14.1780166626\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.022187233\n",
      "range:(5000, 10000) loss= 14.0510139465\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.988483429\n",
      "range:(5000, 10000) loss= 13.9665517807\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 379\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0826091766\n",
      "range:(5000, 10000) loss= 14.035033226\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0552883148\n",
      "range:(5000, 10000) loss= 14.1639480591\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.082075119\n",
      "range:(5000, 10000) loss= 14.1759662628\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0232143402\n",
      "range:(5000, 10000) loss= 14.0548210144\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.992852211\n",
      "range:(5000, 10000) loss= 13.9700870514\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 380\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0856637955\n",
      "range:(5000, 10000) loss= 14.038775444\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0596637726\n",
      "range:(5000, 10000) loss= 14.168885231\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0831384659\n",
      "range:(5000, 10000) loss= 14.1718597412\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0160961151\n",
      "range:(5000, 10000) loss= 14.0470685959\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9840669632\n",
      "range:(5000, 10000) loss= 13.9591035843\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 381\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0728292465\n",
      "range:(5000, 10000) loss= 14.0258712769\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0460042953\n",
      "range:(5000, 10000) loss= 14.1528453827\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0715694427\n",
      "range:(5000, 10000) loss= 14.1677417755\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0202083588\n",
      "range:(5000, 10000) loss= 14.0545024872\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9916086197\n",
      "range:(5000, 10000) loss= 13.9666938782\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 382\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0819625854\n",
      "range:(5000, 10000) loss= 14.039232254\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0639228821\n",
      "range:(5000, 10000) loss= 14.1713981628\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0749397278\n",
      "range:(5000, 10000) loss= 14.1523542404\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.9974479675\n",
      "range:(5000, 10000) loss= 14.0318870544\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9736995697\n",
      "range:(5000, 10000) loss= 13.951212883\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 383\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.065993309\n",
      "range:(5000, 10000) loss= 14.0212917328\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0403118134\n",
      "range:(5000, 10000) loss= 14.1335496902\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0409479141\n",
      "range:(5000, 10000) loss= 14.1212816238\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.975522995\n",
      "range:(5000, 10000) loss= 14.0121107101\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9554862976\n",
      "range:(5000, 10000) loss= 13.9417686462\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 384\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0923757553\n",
      "range:(5000, 10000) loss= 14.0726251602\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0779981613\n",
      "range:(5000, 10000) loss= 14.1150522232\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 13.9931507111\n",
      "range:(5000, 10000) loss= 14.0837211609\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.9979915619\n",
      "range:(5000, 10000) loss= 14.088804245\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0361747742\n",
      "range:(5000, 10000) loss= 13.9700775146\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 385\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0631227493\n",
      "range:(5000, 10000) loss= 14.0165843964\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0665426254\n",
      "range:(5000, 10000) loss= 14.1792354584\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0839414597\n",
      "range:(5000, 10000) loss= 14.124666214\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.9797124863\n",
      "range:(5000, 10000) loss= 14.0352945328\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9891176224\n",
      "range:(5000, 10000) loss= 13.9465379715\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 386\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0334510803\n",
      "range:(5000, 10000) loss= 13.9628009796\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 13.9726047516\n",
      "range:(5000, 10000) loss= 14.0805940628\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0327167511\n",
      "range:(5000, 10000) loss= 14.1342105865\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.9976482391\n",
      "range:(5000, 10000) loss= 14.049706459\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0225877762\n",
      "range:(5000, 10000) loss= 14.0131607056\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 387\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1548461914\n",
      "range:(5000, 10000) loss= 14.117316246\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1596755981\n",
      "range:(5000, 10000) loss= 14.221072197\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0919933319\n",
      "range:(5000, 10000) loss= 14.1247549057\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.971077919\n",
      "range:(5000, 10000) loss= 14.0083045959\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9462337494\n",
      "range:(5000, 10000) loss= 13.9058227539\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 388\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0088882446\n",
      "range:(5000, 10000) loss= 13.9653749466\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0015621185\n",
      "range:(5000, 10000) loss= 14.1129770279\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0505895615\n",
      "range:(5000, 10000) loss= 14.1410560608\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0204982758\n",
      "range:(5000, 10000) loss= 14.0865325928\n",
      "\n",
      "=========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0527601242\n",
      "range:(5000, 10000) loss= 14.0111141205\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 389\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1247272491\n",
      "range:(5000, 10000) loss= 14.0754413605\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.1013975143\n",
      "range:(5000, 10000) loss= 14.1672124863\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.059214592\n",
      "range:(5000, 10000) loss= 14.1249418259\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.9860372543\n",
      "range:(5000, 10000) loss= 14.0202693939\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9545383453\n",
      "range:(5000, 10000) loss= 13.9207258224\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 390\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0455350876\n",
      "range:(5000, 10000) loss= 14.0264101028\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0884218216\n",
      "range:(5000, 10000) loss= 14.1946687698\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.1123561859\n",
      "range:(5000, 10000) loss= 14.1695146561\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0100269318\n",
      "range:(5000, 10000) loss= 14.0215034485\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9414520264\n",
      "range:(5000, 10000) loss= 13.9002275467\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 391\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0203504562\n",
      "range:(5000, 10000) loss= 13.9916639328\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.028673172\n",
      "range:(5000, 10000) loss= 14.1231746674\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0470981598\n",
      "range:(5000, 10000) loss= 14.1410560608\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0189590454\n",
      "range:(5000, 10000) loss= 14.0599498749\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 14.0066204071\n",
      "range:(5000, 10000) loss= 13.9785270691\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 392\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.1024084091\n",
      "range:(5000, 10000) loss= 14.0567064285\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0850563049\n",
      "range:(5000, 10000) loss= 14.1635580063\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0695667267\n",
      "range:(5000, 10000) loss= 14.1314163208\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.9787397385\n",
      "range:(5000, 10000) loss= 14.0030212402\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9378070831\n",
      "range:(5000, 10000) loss= 13.9083395004\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 393\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0240726471\n",
      "range:(5000, 10000) loss= 13.9842319489\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0192251205\n",
      "range:(5000, 10000) loss= 14.1256799698\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0592393875\n",
      "range:(5000, 10000) loss= 14.1473169327\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 14.0156583786\n",
      "range:(5000, 10000) loss= 14.0473241806\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.986620903\n",
      "range:(5000, 10000) loss= 13.9590530396\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 394\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0825996399\n",
      "range:(5000, 10000) loss= 14.0295724869\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0455608368\n",
      "range:(5000, 10000) loss= 14.1302623749\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0547456741\n",
      "range:(5000, 10000) loss= 14.1308994293\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.9834041595\n",
      "range:(5000, 10000) loss= 14.0104122162\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9519262314\n",
      "range:(5000, 10000) loss= 13.9320497513\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 395\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0493392944\n",
      "range:(5000, 10000) loss= 13.9961624146\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0190267563\n",
      "range:(5000, 10000) loss= 14.1242628098\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0647630692\n",
      "range:(5000, 10000) loss= 14.1415996552\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.9945955276\n",
      "range:(5000, 10000) loss= 14.0326194763\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9854516983\n",
      "range:(5000, 10000) loss= 13.94668293\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 396\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0400648117\n",
      "range:(5000, 10000) loss= 13.9794816971\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0046672821\n",
      "range:(5000, 10000) loss= 14.105099678\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0313825607\n",
      "range:(5000, 10000) loss= 14.1089916229\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.9674978256\n",
      "range:(5000, 10000) loss= 14.0033226013\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9509620667\n",
      "range:(5000, 10000) loss= 13.9239521027\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 397\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0348358154\n",
      "range:(5000, 10000) loss= 13.9901628494\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0333652496\n",
      "range:(5000, 10000) loss= 14.1549692154\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0955448151\n",
      "range:(5000, 10000) loss= 14.1483945847\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.9805583954\n",
      "range:(5000, 10000) loss= 14.0067529678\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9539575577\n",
      "range:(5000, 10000) loss= 13.9237852097\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 398\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0280218124\n",
      "range:(5000, 10000) loss= 13.9734563828\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0034446716\n",
      "range:(5000, 10000) loss= 14.1083049774\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.0258998871\n",
      "range:(5000, 10000) loss= 14.090970993\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.951010704\n",
      "range:(5000, 10000) loss= 14.0059947968\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9727334976\n",
      "range:(5000, 10000) loss= 13.9524230957\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 399\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0725250244\n",
      "range:(5000, 10000) loss= 14.0370435715\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0795059204\n",
      "range:(5000, 10000) loss= 14.1581916809\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 14.050160408\n",
      "range:(5000, 10000) loss= 14.1137647629\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.9805545807\n",
      "range:(5000, 10000) loss= 14.0231513977\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.9592313766\n",
      "range:(5000, 10000) loss= 13.913816452\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "epoch: 400\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "current_batch: 1\n",
      "range:(0, 5000) loss= 14.0188522339\n",
      "range:(5000, 10000) loss= 13.974401474\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 2\n",
      "range:(0, 5000) loss= 14.0022420883\n",
      "range:(5000, 10000) loss= 14.0802145004\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 3\n",
      "range:(0, 5000) loss= 13.9677419662\n",
      "range:(5000, 10000) loss= 14.0317220688\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 4\n",
      "range:(0, 5000) loss= 13.8978061676\n",
      "range:(5000, 10000) loss= 13.9480829239\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "current_batch: 5\n",
      "range:(0, 5000) loss= 13.8944416046\n",
      "range:(5000, 10000) loss= 13.8650808334\n",
      "\n",
      "=========================================================================================\n",
      "\n",
      "=================================================================================================\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell. Since, the data used for this task is CIFAR-10, \n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "    \n",
    "    It took me around 5hrs to execute this cell entirely.\n",
    "'''\n",
    "\n",
    "with tf.Session(graph=computation_graph) as sess:\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(new_model_path, \"checkpoint\"))):\n",
    "         # load the weights from the model2\n",
    "        saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(new_model_path))\n",
    "    \n",
    "    else:\n",
    "        # create a new saver\n",
    "        saver = tf.train.Saver(max_to_keep=2)\n",
    "        \n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for ep in range(3 * no_of_epochs, 4 * no_of_epochs):  # start the loop from 10 now\n",
    "        \n",
    "        print \"epoch: \" + str(ep + 1)\n",
    "        print \"=================================================================================================\"\n",
    "        print \"=================================================================================================\"\n",
    "        \n",
    "        for batch_n in range(no_of_batches):  # batches loop\n",
    "            \n",
    "            # retrieve the operations from the graph to be evaluated\n",
    "            loss = sess.graph.get_tensor_by_name(\"loss:0\")\n",
    "            train_op = sess.graph.get_operation_by_name(\"train_op\")\n",
    "            inputs = sess.graph.get_tensor_by_name(\"inputs:0\")\n",
    "            \n",
    "            # generate the batch images and labels\n",
    "            batch_images, batch_labels = generateBatch(os.path.join(data_path, \"data_batch_\" + str(batch_n + 1)))\n",
    "            \n",
    "            min_batch_size = 5000 # we look at only 5000 images at a time since the machine is small\n",
    "            \n",
    "            print \"current_batch: \" + str(batch_n + 1)\n",
    "            \n",
    "            for index in range(len(batch_images) / min_batch_size):\n",
    "                start = index * min_batch_size\n",
    "                end = start + min_batch_size\n",
    "                _, cost = sess.run([train_op, loss], feed_dict={inputs: batch_images[start: end]})\n",
    "                print('range:{} loss= {}'.format((start, end), cost))\n",
    "            \n",
    "            print \"\\n=========================================================================================\\n\"\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0):\n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(new_model_path, \"model2\"), global_step = (ep + 1))\n",
    "        \n",
    "    print \"=================================================================================================\"\n",
    "    print \"=================================================================================================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph = computation_graph) as sess:\n",
    "    # load the weights from the model1\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # instead of global variable initializer, restore the graph:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(new_model_path))\n",
    "    \n",
    "    prediction = sess.graph.get_tensor_by_name(\"prediction:0\")\n",
    "    inputs = sess.graph.get_tensor_by_name(\"inputs:0\")\n",
    "    \n",
    "    random_image = batch_data[np.random.randint(len(batch_data))]\n",
    "    reconstructed_image = sess.run(prediction, feed_dict={inputs: np.array([random_image])})[0]\n",
    "    \n",
    "    # plot the two images with their titles:\n",
    "    plt.figure().suptitle(\"Original Image\")\n",
    "    plt.imshow(random_image, interpolation='none')\n",
    "    \n",
    "    plt.figure().suptitle(\"Reconstructed Image\")\n",
    "    plt.imshow(reconstructed_image, interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
